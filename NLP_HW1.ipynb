{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AAhmadS/NLP_HW1/blob/main/NLP_HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvVmtJhaLMx_"
      },
      "source": [
        "#**Natural Language Processing-Homework1**\n",
        "\n",
        "####**Student name** : Amirahmad Shafiee<br/>**Student number** : 99104027<br/>**Chosen task** : classification\n",
        "In this notebook we aim to do common but rather essential preprocess actions on some text chosen from the persian literature world. After doing so, an NLP task would be operated on the processed text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVCuqcZ0MmIi",
        "outputId": "b6db4b6e-0ae6-47b6-cfe6-1d257bf9f797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3kFSCcFM7j6",
        "outputId": "e7e8d56a-b84d-4a46-e7aa-ea6e2507001e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/.shortcut-targets-by-id/1Ikn75GANDVFyb0klJe4-EBQJikt4_Ob_/NLP\n"
          ]
        }
      ],
      "source": [
        "%cd /content/gdrive/MyDrive/NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldv2jmur72rm"
      },
      "source": [
        "##Essential packages and data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr1hjArriwYv"
      },
      "source": [
        "The data we tend to use in this notebook will be the texts extracted from philosophers books on ethics, like : \n",
        ">**Immanuel Kant's** \"Critique of Pure Reason\"<br/>\n",
        ">**Friedrich Neitzsche's**\"Human all too human\"<br/>\n",
        ">**Georg Wilhelm Friedrich Hegel's**\"Phenomenology of spirit\"<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fggNKpn4nr-"
      },
      "outputs": [],
      "source": [
        "!gdown https://rauterberg.employee.id.tue.nl/lecturenotes/DDM110%20CAS/Kant-1781%20Critique%20of%20Pure%20Reason.pdf\n",
        "!gdown http://www.faculty.umb.edu/gary_zabel/Courses/Marxist_Philosophy/Hegel_and_Feuerbach_files/Hegel-Phenomenology-of-Spirit.pdf\n",
        "!gdown https://drive.google.com/file/d/1x4cM62wac-nEU8X_rEyYLLsWOX67Kqmr/view?usp=share_link"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l0jWZyKjOui"
      },
      "source": [
        "Our data comes in the pdf form. So we need the primary text extractors compatible with pdf format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NW6Arfgd6UFV",
        "outputId": "ea509300-e555-4a91-bff5-eb2d2253d5e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 KB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q openpyxl==3.0.10\n",
        "!pip install -q PyPDF2==3.0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CZqxQvPq7_P7"
      },
      "outputs": [],
      "source": [
        "import random \n",
        "import tqdm\n",
        "import nltk, re\n",
        "from nltk import word_tokenize, sent_tokenize, WhitespaceTokenizer, FreqDist, pos_tag\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "import itertools\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from PyPDF2 import PdfReader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPLBijV-khFZ",
        "outputId": "63149ab3-acb8-4eaa-c6a9-941ce6415aea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_smO9LWLc3R"
      },
      "source": [
        "##Section A: Extracting data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJW-OM3VWD-6"
      },
      "outputs": [],
      "source": [
        "mask_list1 = np.concatenate((np.arange(36),np.array([529]),np.arange(530,631)))\n",
        "with open('pdf_mask_list_heg.npy','wb') as f:\n",
        "  np.save(f,mask_list1)\n",
        "\n",
        "mask_list2 = np.concatenate((np.arange(103),np.array([129,156,625]),np.arange(705,785)))\n",
        "with open('pdf_mask_list_kan.npy','wb') as f:\n",
        "  np.save(f,mask_list2)\n",
        "\n",
        "mask_list3 = np.concatenate((np.arange(30),np.array([230,231,232,233,325]),np.arange(421,425)))\n",
        "with open('pdf_mask_list_nei.npy','wb') as f:\n",
        "  np.save(f,mask_list3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lIH0GSflq7vR"
      },
      "outputs": [],
      "source": [
        "name_list=[\"heg\",\"kan\",\"nei\"]\n",
        "address_list=[\"Hegel-Phenomenology-of-Spirit.pdf\",\"Kant-1781%20Critique%20of%20Pure%20Reason.pdf\",\"Human, All Too Human_ A Book for Free Spirits - PART I + PART II - Friedrich Nietzsche - PDF.pdf\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUk6XvkY6lLt"
      },
      "outputs": [],
      "source": [
        "for i in range(3):\n",
        "  reader = PdfReader(address_list[i])\n",
        "  number_of_pages = len(reader.pages)\n",
        "\n",
        "  with open(f'pdf_mask_list_{name_list[i]}.npy','rb') as f:\n",
        "    mask_list = np.load(f)\n",
        "    text_pages_index = np.delete(np.arange(number_of_pages),mask_list)\n",
        "\n",
        "  text = \"\"\n",
        "  for ind in text_pages_index:\n",
        "    text+=reader.pages[int(ind)].extract_text()\n",
        "\n",
        "  with open(f'final_text_{name_list[i]}.txt','wb') as f:\n",
        "    pickle.dump(text,f)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XA4bpKhtSdgK"
      },
      "outputs": [],
      "source": [
        "texts = []\n",
        "for x in name_list:\n",
        "  with open(f'final_text_{x}.txt','rb') as f:\n",
        "    text = pickle.load(f)\n",
        "    texts.append(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1vunFgSITsAH"
      },
      "outputs": [],
      "source": [
        "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "sentences = [sent_detector.tokenize(text) for text in texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srawgE3j_Z0U",
        "outputId": "6b13cb55-0411-433e-b2fc-51508b02f064"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-605b0474b0a1>:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  data = data.append(sentences[i],ignore_index = True)\n",
            "<ipython-input-9-605b0474b0a1>:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  labels = labels.astype(np.int)\n"
          ]
        }
      ],
      "source": [
        "data = pd.DataFrame()\n",
        "labels=np.array([])\n",
        "labels_auth = []\n",
        "for i in range(3):\n",
        "  data = data.append(sentences[i],ignore_index = True)\n",
        "  labels = np.concatenate((labels, (i*np.ones(len(sentences[i])))))\n",
        "\n",
        "labels = labels.astype(np.int)\n",
        "labels_auth = [name_list[i] for i in labels]\n",
        "data.rename(columns = {0 : \"sentences\"}, inplace=True)\n",
        "data[\"labels\"] = labels\n",
        "data[\"author\"] = labels_auth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "UE00NsgzUkom",
        "outputId": "523f6fc3-4f99-40b2-8088-c88e62beebd7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               sentences  labels author\n",
              "19435  -A good educator knows cases in which he is pr...       2    nei\n",
              "6698   In the mystery Df \\nbread and wine, it appropr...       0    heg\n",
              "13328  A55I/B 579 Suppose now that one could say reas...       1    kan\n",
              "6151   But, in this picture-thinking, reaHtydoes \\nno...       0    heg\n",
              "20156  Further stages of mor­\\nality, and thus of mea...       2    nei"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-93c714e7-beb2-4fd9-a3f4-89cfef448834\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentences</th>\n",
              "      <th>labels</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>19435</th>\n",
              "      <td>-A good educator knows cases in which he is pr...</td>\n",
              "      <td>2</td>\n",
              "      <td>nei</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6698</th>\n",
              "      <td>In the mystery Df \\nbread and wine, it appropr...</td>\n",
              "      <td>0</td>\n",
              "      <td>heg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13328</th>\n",
              "      <td>A55I/B 579 Suppose now that one could say reas...</td>\n",
              "      <td>1</td>\n",
              "      <td>kan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6151</th>\n",
              "      <td>But, in this picture-thinking, reaHtydoes \\nno...</td>\n",
              "      <td>0</td>\n",
              "      <td>heg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20156</th>\n",
              "      <td>Further stages of mor­\\nality, and thus of mea...</td>\n",
              "      <td>2</td>\n",
              "      <td>nei</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-93c714e7-beb2-4fd9-a3f4-89cfef448834')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-93c714e7-beb2-4fd9-a3f4-89cfef448834 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-93c714e7-beb2-4fd9-a3f4-89cfef448834');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POGtw5EP67Gs",
        "outputId": "c6efe3d5-061c-48fa-8e49-1ca607fef7c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "heg text consists of 1251449 characters, in other words the volume of the text is about 1.25 megs\n",
            "also 7317 sentences have been spotted\n",
            "kan text consists of 1642328 characters, in other words the volume of the text is about 1.64 megs\n",
            "also 8148 sentences have been spotted\n",
            "nei text consists of 1102859 characters, in other words the volume of the text is about 1.10 megs\n",
            "also 5997 sentences have been spotted\n"
          ]
        }
      ],
      "source": [
        "for txt,x,sen in zip(texts,name_list,sentences):\n",
        "  print(f\"{x} text consists of {len(txt)} characters, in other words the volume of the text is about {len(txt)/1e6:.2f} megs\\nalso {len(sen)} sentences have been spotted\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "bPkCdEDF0s7s"
      },
      "outputs": [],
      "source": [
        "class distributions:\n",
        "\n",
        "  def minmax(data, col_end = 'sentence_len_by_words', col_beg = 'sentences'):\n",
        "    data[col_end] = data[col_beg].apply(lambda t: len(word_tokenize(t)))\n",
        "\n",
        "    min_max_len = data[col_end].min(), data[col_end].max()\n",
        "    print(f'Min: {min_max_len[0]} \\tMax: {min_max_len[1]}')\n",
        "\n",
        "    return\n",
        "\n",
        "  def word_freq(data, col_end = \"sentence_len_by_words\"):\n",
        "\n",
        "      fig = go.Figure()\n",
        "\n",
        "      fig.add_trace(go.Histogram(\n",
        "          x=data[col_end]\n",
        "      ))\n",
        "\n",
        "      fig.update_layout(\n",
        "          title_text='Distribution of word counts within data',\n",
        "          xaxis_title_text='Word Count',\n",
        "          yaxis_title_text='Frequency',\n",
        "          bargap=0.2,\n",
        "          bargroupgap=0.2)\n",
        "\n",
        "      fig.show()\n",
        "\n",
        "      return\n",
        "\n",
        "  def data_gl_than(data, less_than=25.0, greater_than=3.0, col='sentence_len_by_words'):\n",
        "      data_length = data[col].values\n",
        "\n",
        "      data_glt = sum([1 for length in data_length if greater_than < length <= less_than])\n",
        "\n",
        "      data_glt_rate = (data_glt / len(data_length)) * 100\n",
        "\n",
        "      print(f'sentences with word length of greater than {greater_than} and less than {less_than} includes {data_glt_rate:.2f}% of the whole!')\n",
        "\n",
        "      return\n",
        "\n",
        "  def full_df_run(data, col_begin = 'sentences'):\n",
        "    distributions.minmax(data)\n",
        "    distributions.word_freq(data)\n",
        "    distributions.data_gl_than(data)\n",
        "\n",
        "\n",
        "  def word_and_freq(data, col = \"tokenized_sents\"):\n",
        "    tokenized_sent = data[\"tokenized_sents\"]\n",
        "    mp_freqdist = FreqDist(itertools.chain(*tokenized_sent))\n",
        "    top20words=mp_freqdist.most_common(20)\n",
        "    print ('%-16s' % 'word', '%-16s' % 'Frequency','%-16s' %  '% of the total')\n",
        "    for topword in top20words:\n",
        "        percent=(topword[1]/len(tokenized_sent))*100\n",
        "        print ('%-16s' % topword[0], '%-16s' % topword[1],'%-16s' %  percent)\n",
        "    \n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "distributions.full_df_run(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "id": "i-2RSVgnVH2S",
        "outputId": "a22ea481-b24d-4190-85de-0eb0f2273b29"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min: 1 \tMax: 340\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"47881cb9-fc59-405d-b301-1bebb9fde1d7\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"47881cb9-fc59-405d-b301-1bebb9fde1d7\")) {                    Plotly.newPlot(                        \"47881cb9-fc59-405d-b301-1bebb9fde1d7\",                        [{\"x\":[7,42,36,54,62,64,57,4,38,2,50,41,24,58,19,47,8,3,25,44,2,24,42,68,42,44,29,25,62,37,2,89,33,33,2,17,38,25,52,61,2,64,30,70,46,2,94,27,27,53,85,68,2,67,15,20,26,79,28,39,18,20,45,16,11,2,18,47,72,29,73,67,3,23,34,13,70,5,23,32,2,25,16,29,28,19,39,28,45,35,38,58,21,54,34,2,25,33,27,29,62,33,21,58,31,51,42,40,17,27,2,38,40,55,49,55,50,61,3,2,39,36,45,34,51,2,46,49,25,41,2,52,34,36,65,33,25,11,2,6,15,37,11,53,66,32,43,3,21,40,35,30,67,36,12,41,2,17,32,40,17,46,58,14,2,30,25,14,29,17,65,20,28,10,61,96,2,68,10,30,27,71,11,52,2,42,81,27,20,19,54,32,2,22,16,18,15,26,46,31,32,51,21,30,22,20,20,63,63,56,27,2,23,16,3,33,92,2,40,32,28,46,55,44,42,34,28,38,38,2,32,11,18,72,93,51,42,8,12,2,17,7,28,29,35,34,66,2,17,46,37,22,35,2,34,40,21,28,36,30,62,33,17,33,15,72,14,27,30,2,65,33,52,70,21,51,42,50,12,33,2,31,81,25,2,36,17,22,4,21,38,30,33,19,12,22,63,31,21,40,32,31,47,32,10,21,24,43,20,2,65,44,15,34,2,43,22,18,55,35,28,7,12,17,23,20,55,24,33,36,18,7,10,40,2,42,24,38,20,2,47,17,34,40,2,48,45,46,22,19,22,35,19,41,30,13,13,5,36,57,2,24,28,28,14,61,2,40,63,25,32,36,2,55,7,14,38,29,28,33,18,5,70,59,16,24,40,13,2,24,25,20,83,48,38,38,12,46,2,60,28,27,54,24,19,38,14,12,37,47,2,19,3,29,17,28,9,2,31,28,33,29,39,27,50,11,43,53,33,2,32,2,73,2,38,58,53,29,43,25,44,46,22,3,45,7,72,113,37,54,38,36,25,52,52,31,68,63,36,2,38,61,2,13,48,10,47,14,30,46,37,34,42,20,67,13,32,8,32,34,61,24,2,6,23,23,14,43,8,88,49,25,61,68,2,42,25,12,18,24,37,17,38,21,16,18,77,20,2,24,42,43,56,2,43,23,29,34,18,11,36,22,2,34,2,30,7,6,9,57,2,45,20,11,12,54,37,25,25,28,20,8,44,47,50,25,42,37,54,22,2,11,50,27,15,24,22,34,31,58,11,80,30,24,24,93,2,87,13,46,52,1,2,36,39,4,46,83,24,90,2,32,52,10,31,9,38,2,43,33,22,25,26,12,27,21,21,48,25,11,19,45,56,34,30,33,50,16,6,46,46,44,2,25,47,20,34,94,26,55,28,2,83,60,50,67,2,33,66,21,36,63,52,94,12,27,26,2,61,42,43,22,31,17,58,18,7,78,111,48,40,96,21,22,37,29,19,42,48,2,73,22,42,22,46,83,45,49,56,36,62,17,36,67,44,36,44,2,53,19,35,32,79,2,20,62,45,11,2,175,39,54,33,40,22,74,43,11,33,45,33,20,2,111,2,25,54,21,81,33,86,31,44,52,46,31,75,2,24,36,49,33,45,48,55,2,40,29,39,11,25,32,17,31,7,27,59,71,67,3,33,41,55,43,2,24,51,43,18,37,2,24,33,22,43,2,23,29,6,47,28,37,31,21,15,26,18,57,2,72,41,39,60,49,16,33,54,30,78,2,38,35,23,10,27,18,21,39,13,18,19,2,28,29,26,58,23,46,31,47,46,31,21,44,30,31,30,2,32,2,38,19,26,13,72,22,17,40,24,14,3,63,34,20,30,16,100,60,17,27,42,2,36,16,54,44,17,6,2,58,86,17,35,2,45,36,2,17,43,8,8,5,16,30,24,2,10,32,49,36,6,56,38,17,2,17,10,12,45,70,2,19,9,33,37,19,2,32,83,2,36,45,19,10,27,45,50,19,19,41,9,28,4,71,30,18,16,31,39,74,2,87,42,7,9,30,2,63,7,47,5,68,66,2,52,35,37,21,18,18,2,9,17,34,31,11,38,2,36,29,20,50,38,47,24,43,27,26,21,25,2,42,15,22,51,80,2,34,39,57,30,23,11,56,22,19,22,38,4,28,54,67,21,3,49,47,28,49,8,53,48,9,28,47,42,38,88,5,46,3,11,24,17,32,36,50,37,57,36,46,9,3,35,30,14,43,6,38,4,54,24,26,34,18,51,16,57,33,19,77,24,20,33,25,3,41,57,93,39,37,25,3,29,92,29,41,64,3,30,26,22,32,37,20,62,3,12,30,15,37,17,21,25,17,52,23,64,56,22,50,13,38,3,26,29,16,28,36,27,52,12,19,57,42,2,55,46,31,43,56,2,27,28,55,20,31,16,27,45,16,25,25,46,2,39,20,29,22,35,29,29,33,36,2,65,47,71,2,17,26,19,42,31,26,23,24,41,20,20,28,2,39,36,40,50,33,9,17,49,34,2,49,59,56,2,33,53,2,23,43,30,12,2,51,69,5,2,35,81,27,36,2,28,24,23,31,31,11,10,59,74,3,54,45,66,75,52,79,49,46,39,73,49,16,26,80,2,12,40,65,22,36,31,23,42,53,2,18,53,29,29,36,2,34,40,4,53,28,26,27,50,72,13,13,18,47,31,37,53,12,36,9,27,9,51,40,53,9,28,43,34,34,57,25,52,24,28,118,46,2,47,56,96,15,22,36,43,19,14,24,2,66,34,32,34,15,39,46,19,42,2,39,95,57,68,8,80,41,62,14,102,2,26,41,11,55,39,54,19,42,47,60,29,39,2,41,63,42,2,50,39,26,18,33,20,32,20,13,33,57,51,34,19,24,2,101,2,70,2,23,22,37,104,74,34,42,12,94,31,2,44,30,8,71,12,26,39,31,33,26,17,41,30,16,35,13,35,46,26,51,78,27,7,25,87,29,17,42,2,23,28,29,15,14,15,22,31,36,41,13,69,33,18,58,45,2,20,39,33,45,49,70,11,2,73,30,52,60,24,22,49,17,39,20,35,13,47,7,39,21,17,2,27,29,20,92,27,23,73,29,2,49,40,36,26,54,57,7,44,41,54,29,17,2,35,42,26,19,24,29,14,37,4,41,40,29,2,37,33,65,21,23,20,28,90,49,2,30,37,18,38,33,12,45,18,2,44,40,25,20,39,25,71,70,35,2,74,70,62,28,40,81,51,63,40,40,73,2,83,18,47,45,22,13,34,15,22,2,32,47,14,32,69,23,25,32,18,21,2,71,17,106,30,36,17,24,21,26,41,61,72,22,16,2,23,45,43,15,32,82,71,50,80,41,26,29,5,22,45,48,26,39,32,53,16,23,63,35,64,70,52,18,80,4,6,19,13,58,32,16,26,25,94,64,39,2,15,18,30,48,47,25,15,7,30,76,40,6,74,26,16,6,38,23,31,5,35,11,55,40,69,21,38,2,35,10,60,56,36,45,40,2,32,35,36,26,28,2,41,13,19,72,32,19,48,20,50,14,16,36,18,41,17,23,32,42,57,23,2,45,33,47,15,2,25,27,38,2,61,55,18,40,42,28,37,56,28,45,26,9,2,21,6,31,26,21,29,32,45,2,7,29,45,18,13,51,54,14,31,18,43,30,23,2,17,53,2,8,17,44,2,19,68,2,52,25,61,13,39,20,2,39,2,40,30,46,48,50,9,3,28,39,12,2,21,11,24,14,17,42,69,25,40,56,2,59,23,19,32,25,34,80,33,56,34,2,33,40,33,49,69,41,2,18,21,38,14,5,65,10,3,21,20,24,35,65,61,55,87,29,13,20,45,16,2,44,20,13,23,34,25,24,37,13,2,23,47,16,17,17,2,16,21,62,2,14,22,29,15,27,43,29,37,24,24,27,2,56,15,44,19,25,28,26,37,30,2,44,35,18,35,22,43,40,41,35,27,67,35,44,60,20,47,30,86,55,49,66,17,146,56,40,58,2,28,37,2,24,19,9,37,16,12,14,91,37,40,2,31,29,14,26,18,21,36,86,25,44,24,9,29,20,49,3,20,21,28,7,28,13,41,66,43,90,39,2,23,48,89,2,40,56,58,12,30,31,39,26,2,26,22,73,40,16,65,63,47,36,12,37,44,21,20,32,31,42,53,2,11,20,25,46,13,33,24,47,2,62,47,29,24,2,39,27,42,29,3,67,17,89,2,33,42,24,13,6,49,3,28,24,15,26,18,34,30,15,56,3,28,69,42,73,28,4,32,47,67,72,14,40,45,3,64,57,7,29,2,71,25,34,21,32,34,2,69,9,51,40,39,3,84,47,43,75,93,39,50,30,57,18,45,60,15,2,31,53,6,15,52,8,48,71,4,2,104,2,48,22,41,62,29,46,3,19,61,52,11,43,78,2,34,53,34,44,49,33,17,40,35,68,38,42,29,18,17,35,29,2,27,11,2,38,27,31,64,109,2,33,26,42,4,35,55,36,2,23,37,27,30,15,10,29,87,2,56,31,12,125,31,30,31,34,28,59,27,28,36,81,26,25,30,65,2,4,12,22,11,32,29,58,61,42,2,26,46,58,19,4,58,91,2,19,31,33,70,33,9,24,59,40,12,41,46,33,43,9,2,39,25,25,36,18,17,42,45,28,9,31,9,4,34,2,9,21,39,82,33,37,18,8,56,12,42,52,49,37,2,38,13,31,21,32,44,9,45,16,13,22,22,30,48,2,56,32,4,4,48,76,2,15,17,69,28,29,57,24,5,74,39,100,59,5,2,57,22,72,2,4,54,35,21,33,12,6,4,23,9,23,2,60,41,58,50,2,5,22,38,31,57,39,73,10,27,27,14,36,24,19,2,55,7,38,48,38,19,2,28,68,69,42,38,50,61,42,41,18,29,2,23,45,47,33,67,46,34,36,63,24,28,29,51,47,4,23,2,30,78,56,64,25,42,2,50,26,9,38,44,68,2,37,30,46,28,15,41,36,70,30,12,2,65,10,4,35,67,42,141,32,44,28,2,50,10,32,23,18,65,82,26,36,23,40,100,51,27,2,4,20,2,30,80,2,25,59,28,46,2,21,22,98,2,39,61,43,64,53,62,87,21,44,74,4,27,36,43,38,27,2,33,33,39,53,33,25,2,2,51,49,48,35,38,30,67,28,40,2,21,53,35,10,42,20,11,20,28,2,11,4,21,11,27,25,14,42,44,45,50,19,40,23,57,13,2,37,42,24,42,42,22,53,23,9,32,23,97,2,100,11,4,27,2,83,31,61,55,17,21,2,17,44,2,64,39,19,29,24,34,2,28,21,52,50,70,2,10,66,2,43,51,4,16,10,46,25,2,89,2,64,55,17,53,29,11,28,35,76,27,78,3,27,21,21,61,75,19,87,4,49,36,8,73,19,47,63,2,76,39,2,80,2,33,37,36,52,2,56,57,35,61,20,48,3,4,3,43,36,67,48,48,40,20,88,41,2,100,22,31,20,2,18,27,96,36,2,22,81,31,45,4,10,45,59,50,2,22,7,23,20,9,51,26,4,14,40,25,2,72,48,51,2,104,30,67,19,88,24,4,27,40,53,8,73,2,56,2,50,15,57,2,44,24,68,44,74,43,89,28,54,28,33,28,2,40,4,48,57,47,2,33,59,33,18,2,22,52,37,14,2,51,56,46,21,17,91,39,38,22,18,7,34,65,30,21,9,4,45,2,66,16,71,59,25,42,73,57,91,36,47,57,2,32,26,18,15,49,36,34,11,16,27,16,4,14,37,23,45,60,32,43,2,66,35,22,39,68,48,9,17,52,55,70,15,24,43,2,79,27,2,39,4,20,90,35,67,39,2,52,67,44,17,22,91,2,97,2,78,39,41,89,44,4,31,2,14,6,43,37,37,31,3,2,30,30,21,17,84,34,47,2,30,6,45,33,36,44,39,27,38,26,10,24,23,17,2,27,26,16,4,36,31,2,18,30,111,64,69,2,104,2,37,57,30,34,38,2,33,28,40,2,31,79,63,4,4,49,53,44,28,31,75,2,115,64,29,45,2,31,14,74,26,14,5,67,18,2,50,41,32,27,4,46,53,35,62,47,50,59,43,58,34,4,45,40,20,41,121,38,83,50,30,15,4,30,21,107,2,16,78,2,36,46,33,2,28,24,66,52,9,55,4,57,34,54,38,19,63,89,27,4,50,21,4,65,64,32,31,2,46,30,20,52,46,50,168,32,18,30,2,23,34,26,70,18,37,4,21,70,47,25,2,39,26,36,44,11,22,27,76,21,15,79,3,32,109,22,50,2,75,25,12,86,3,4,32,27,13,10,46,18,15,29,100,68,27,43,20,47,90,2,45,26,36,13,18,62,49,2,53,22,44,12,30,13,4,29,73,57,27,38,2,22,25,17,57,2,20,58,18,18,12,31,70,79,2,57,38,6,31,41,69,30,46,11,4,74,29,39,43,3,15,28,2,39,81,70,71,31,83,52,20,2,28,46,46,81,81,17,4,16,41,46,39,3,49,16,13,29,34,17,24,2,12,15,15,35,2,18,11,29,16,39,34,60,88,2,89,64,60,72,4,39,2,32,93,55,43,99,14,36,47,40,27,34,47,33,2,30,24,42,24,45,35,40,2,74,45,4,60,43,17,41,22,53,62,49,105,2,58,18,46,2,28,13,31,48,56,34,54,163,2,30,4,29,29,23,49,56,75,77,46,28,2,28,123,62,25,42,42,2,12,42,23,18,29,13,34,51,2,24,4,39,14,57,20,13,54,20,35,24,78,2,32,17,11,41,6,9,33,16,40,29,19,19,40,18,79,23,2,14,12,45,54,68,14,3,4,14,1,2,76,45,76,55,26,61,77,38,23,12,19,13,53,15,11,35,30,42,2,25,27,30,28,47,82,3,4,63,2,23,71,91,35,43,2,71,62,54,3,23,6,36,45,31,41,57,43,34,66,38,24,8,4,33,2,13,20,33,25,2,49,2,28,45,29,30,2,65,39,20,49,41,33,2,21,37,23,65,3,53,23,2,76,26,37,41,17,4,21,35,24,67,42,12,97,65,2,24,69,33,50,48,39,11,21,50,5,75,40,58,21,55,3,4,43,17,38,23,2,12,55,40,27,33,22,41,18,30,44,2,9,9,29,54,16,53,43,11,20,16,40,53,75,18,38,28,8,63,2,50,37,43,13,20,2,24,57,48,24,38,54,29,2,16,31,26,22,2,12,29,34,36,2,38,28,2,50,31,68,20,4,10,24,88,2,29,61,25,32,22,38,23,2,28,58,35,7,65,39,2,13,17,30,20,54,74,42,24,43,16,55,13,3,4,3,34,36,56,26,49,60,30,2,95,24,80,59,2,23,35,35,27,39,20,3,49,57,65,21,37,4,33,28,2,55,61,27,44,8,40,48,39,44,31,33,6,32,21,33,118,36,10,2,15,33,49,63,50,30,4,12,2,47,25,55,73,11,25,41,47,32,37,25,36,26,19,67,72,2,34,44,2,27,41,45,33,47,33,21,4,15,2,25,27,34,22,46,24,50,26,15,2,52,26,26,65,30,33,6,25,42,104,2,32,13,58,52,15,55,19,33,48,27,4,17,65,88,29,37,35,47,2,55,26,39,44,77,51,57,24,2,41,34,33,64,24,80,2,30,8,4,53,98,40,26,35,30,18,51,67,2,43,37,84,2,39,33,49,24,63,22,65,2,47,30,2,4,15,23,30,25,41,47,56,64,33,32,46,22,45,29,31,2,35,25,47,34,7,46,25,25,2,16,14,37,27,2,51,7,4,42,40,46,143,2,31,26,46,68,24,2,29,33,90,90,3,45,24,31,57,36,55,44,4,5,32,16,23,53,4,70,51,44,57,15,29,56,58,26,43,30,15,30,2,59,27,18,11,34,24,2,24,56,22,61,14,27,4,9,15,26,58,19,37,26,57,59,43,2,40,60,27,40,24,22,31,16,13,17,40,75,70,31,6,70,23,4,21,2,44,53,26,23,77,43,2,84,18,21,35,21,44,2,70,22,41,43,24,39,2,58,55,57,48,4,18,42,19,44,57,2,39,37,25,75,38,17,3,42,64,77,73,51,41,25,3,97,20,34,2,21,45,2,4,31,44,70,51,87,2,39,60,38,64,28,2,26,36,27,41,42,39,39,2,29,15,22,52,51,50,56,6,4,17,55,2,43,31,18,80,53,84,18,34,105,39,30,27,39,33,2,31,58,100,26,29,42,19,4,42,4,61,25,74,46,28,45,2,5,16,44,30,39,16,2,42,44,21,21,71,43,75,36,39,2,37,38,20,2,13,9,4,53,55,2,8,20,22,7,49,27,54,33,29,36,17,43,58,44,39,16,19,47,2,12,59,35,6,11,49,77,44,17,4,21,56,40,25,2,28,21,38,30,28,5,2,60,2,34,8,17,8,45,65,19,39,40,19,49,31,56,20,60,2,14,23,17,12,20,11,2,4,26,2,20,43,52,45,27,34,34,98,3,27,19,40,27,28,9,30,28,31,4,25,33,25,2,41,17,33,57,2,86,2,4,3,33,43,33,91,2,79,19,59,2,40,43,58,34,21,25,52,44,2,13,86,12,19,6,12,23,3,40,33,4,22,27,34,73,36,44,32,48,40,19,38,25,20,4,3,3,34,30,38,28,30,28,25,41,32,8,31,27,2,24,42,4,31,48,33,43,33,25,2,16,11,19,24,46,40,53,19,15,21,4,63,16,51,2,23,37,44,2,10,22,49,116,2,45,4,4,19,2,4,5,16,22,48,34,23,2,43,19,18,57,56,49,5,4,10,12,66,56,42,2,26,78,33,20,53,2,25,28,4,24,33,2,18,62,2,29,32,35,40,10,80,3,83,11,38,49,65,71,13,43,34,46,61,54,39,4,46,4,51,39,2,40,52,57,28,39,127,58,57,41,112,41,2,17,71,32,10,4,49,18,2,17,30,17,23,2,34,20,30,19,41,61,23,34,10,67,52,21,28,2,25,51,28,17,52,57,35,36,12,4,56,2,21,18,20,40,78,45,45,18,66,45,96,62,32,61,21,24,30,41,18,24,37,41,5,2,4,25,31,21,27,2,34,23,46,49,28,2,22,24,94,41,59,19,36,54,2,48,27,63,33,34,54,25,21,19,4,8,74,62,2,24,32,53,35,63,29,85,15,8,5,5,27,25,10,17,12,55,60,2,31,23,2,46,97,16,24,4,5,28,26,47,87,28,27,45,62,63,2,52,71,42,31,59,43,19,59,81,2,84,4,3,62,32,55,10,16,39,66,47,22,86,4,99,31,41,26,2,30,39,36,16,31,59,59,58,48,4,32,2,41,53,37,43,29,14,33,18,47,46,33,2,29,2,13,36,21,20,38,78,37,43,2,31,28,26,16,23,76,10,41,28,105,69,54,41,2,32,25,25,30,41,25,15,36,40,32,71,25,2,54,27,36,40,33,27,4,23,26,53,55,42,20,78,47,55,39,29,41,53,27,28,19,51,12,2,32,45,28,32,43,25,2,4,6,49,30,3,48,27,42,21,41,2,16,8,25,2,16,15,19,43,16,32,55,54,2,40,77,21,19,45,32,51,30,30,4,28,7,41,31,46,3,52,71,39,17,28,20,23,40,54,25,2,15,30,20,26,27,50,18,9,34,30,28,2,21,18,30,26,22,12,4,26,44,4,3,26,33,47,37,56,67,47,14,26,35,20,18,13,2,25,36,2,27,26,10,36,2,22,42,47,62,27,34,9,4,53,141,31,48,72,29,36,43,2,7,10,37,52,47,19,54,2,37,33,22,19,20,47,53,25,8,4,18,2,13,23,51,43,23,25,22,53,82,20,90,2,19,15,46,31,7,33,22,46,32,19,17,18,42,53,24,34,50,5,43,39,133,58,28,70,58,2,47,45,49,11,64,2,34,13,32,38,32,12,40,47,13,31,4,18,48,39,9,2,45,35,12,24,16,14,26,30,39,44,34,35,2,37,17,40,31,15,36,55,35,51,2,51,19,45,37,50,2,4,3,23,14,8,36,41,38,52,22,2,74,22,18,41,40,27,24,10,2,38,13,47,58,2,15,129,2,41,45,44,21,4,11,25,61,11,23,2,59,29,47,2,30,38,15,32,35,35,2,21,22,54,22,32,11,82,35,2,32,18,67,2,64,25,28,3,4,3,32,51,29,24,37,40,2,18,62,17,36,4,83,24,30,48,40,61,38,2,28,26,53,60,99,42,27,3,13,4,82,50,27,29,42,9,45,30,18,4,32,9,66,99,61,27,39,30,69,2,23,60,27,80,20,4,21,34,6,28,5,72,46,15,2,2,61,73,37,10,12,4,26,27,19,61,4,11,43,44,46,6,20,3,34,32,8,44,21,4,3,38,20,45,4,8,27,24,30,24,46,32,12,3,15,30,97,2,39,22,18,47,14,4,94,17,32,24,86,47,19,2,50,53,20,71,4,18,35,25,31,22,60,66,30,21,74,50,40,33,64,63,43,44,48,37,27,27,22,36,2,45,17,4,90,56,53,34,3,71,42,21,34,20,57,9,4,2,91,2,120,60,7,35,32,44,2,14,52,25,4,32,71,2,25,60,61,45,49,39,51,50,40,33,42,23,28,2,6,15,26,17,5,50,27,30,47,64,18,16,4,15,18,70,41,22,36,2,68,2,41,10,15,59,37,40,9,3,11,34,67,14,64,19,41,57,19,37,53,20,45,4,13,27,40,40,11,23,22,3,36,63,76,2,31,10,47,3,26,24,56,59,2,61,39,31,2,37,40,42,2,60,26,4,23,32,64,71,63,2,42,40,17,2,12,38,41,22,2,24,34,18,22,48,49,54,53,42,15,8,19,4,31,8,11,48,27,26,4,23,24,2,4,31,28,30,2,34,82,40,73,2,16,77,49,48,44,22,36,2,9,82,59,29,45,2,17,3,5,11,13,12,12,15,49,24,62,59,2,14,21,19,22,35,28,58,21,4,18,15,64,54,2,45,41,34,32,22,52,32,31,29,5,4,12,16,25,62,53,2,27,25,23,2,35,29,53,2,2,30,33,40,7,4,26,21,20,41,19,31,20,19,32,41,30,24,58,40,60,25,4,39,2,43,62,47,21,28,40,17,45,38,36,15,33,67,18,9,21,19,2,47,22,31,26,12,2,32,34,34,67,31,22,11,32,8,4,38,9,43,83,2,30,52,28,2,16,19,46,29,23,47,24,5,12,20,75,2,69,26,74,26,19,56,46,21,13,30,39,58,105,24,57,44,83,2,18,46,10,63,68,2,57,68,15,66,86,84,4,13,33,2,12,55,9,8,9,16,11,28,36,14,37,28,41,54,2,51,47,12,24,44,35,29,75,2,30,23,12,21,16,32,95,15,34,31,4,11,46,2,42,9,54,27,82,18,52,37,27,23,9,37,48,26,3,24,33,50,2,26,75,7,11,89,2,52,57,23,4,62,18,2,24,34,9,14,57,73,16,2,26,30,46,33,22,17,30,64,52,10,43,52,26,2,53,44,43,27,59,4,10,49,11,43,7,45,2,39,97,15,31,16,59,2,16,88,14,59,5,22,35,56,24,35,2,47,51,99,17,33,23,14,2,44,29,43,2,33,64,31,38,57,68,2,30,47,44,65,14,42,31,59,33,2,6,34,13,4,33,11,34,45,32,30,3,52,36,8,53,52,2,33,49,38,26,5,22,2,43,36,18,42,13,2,45,37,13,24,42,2,30,23,4,31,53,40,32,103,42,32,81,96,4,27,54,22,19,51,53,2,68,36,28,4,48,77,15,4,12,64,60,13,167,35,3,62,35,55,27,32,31,46,20,22,20,4,17,11,74,43,13,2,9,8,33,25,58,4,44,2,23,76,50,31,10,82,43,39,2,9,52,40,40,68,2,21,64,41,68,62,2,61,33,4,18,40,2,78,67,2,26,42,27,29,76,53,61,67,33,2,41,46,39,33,2,40,55,40,44,4,21,23,58,37,29,2,29,20,26,52,45,71,63,2,25,25,25,21,27,48,57,2,35,86,48,2,46,23,4,70,86,39,85,47,85,50,38,47,79,26,33,15,29,25,13,2,15,39,49,40,15,2,4,9,3,25,17,46,48,17,16,50,2,13,17,73,70,67,3,31,19,14,26,2,8,9,28,21,19,22,77,16,73,2,23,15,28,4,18,28,2,37,10,55,55,51,2,17,33,62,37,53,40,19,11,40,4,36,14,85,38,20,5,25,4,16,10,50,32,35,19,18,4,28,23,38,23,62,16,31,31,45,37,21,33,30,12,23,24,7,50,35,71,3,37,2,39,24,22,24,20,40,2,55,31,40,4,10,45,42,2,23,23,50,67,30,3,21,24,25,34,83,4,41,24,53,8,2,37,48,34,2,23,24,20,42,11,2,12,53,18,18,9,4,33,41,2,31,52,44,17,7,40,47,11,77,4,52,24,8,33,10,15,18,16,69,3,39,64,17,4,22,8,27,2,20,17,47,45,3,45,43,41,7,48,25,26,38,20,36,2,23,23,60,37,53,21,12,2,52,35,28,24,58,82,30,43,57,23,2,33,31,21,4,31,15,36,19,40,92,41,7,21,2,25,18,29,30,32,63,3,33,39,30,24,47,2,25,49,10,24,4,26,13,62,31,20,31,11,19,4,16,40,36,13,53,15,38,57,31,27,8,36,22,36,31,51,2,22,25,20,30,17,40,27,2,35,28,41,8,18,47,26,31,16,6,4,34,2,19,24,9,4,9,66,50,33,10,20,34,41,36,8,4,55,21,33,47,13,2,44,48,31,18,15,56,21,2,76,52,54,10,25,4,4,12,9,11,50,16,40,28,68,3,62,27,53,152,38,55,48,43,20,46,26,5,22,76,23,30,26,4,15,99,33,2,48,36,21,23,48,26,52,35,33,15,39,15,2,56,56,22,2,17,27,37,55,38,16,62,16,29,37,4,42,46,25,22,38,50,2,55,2,31,33,57,96,80,12,2,43,11,48,53,30,2,34,28,26,37,33,55,20,4,24,2,22,29,37,36,20,31,41,40,46,23,68,35,2,38,23,65,42,36,22,2,49,50,41,52,49,18,4,48,57,2,21,20,40,39,22,60,50,15,4,21,49,40,51,30,24,2,16,22,82,53,36,49,57,53,12,23,4,12,17,42,11,2,78,34,53,71,15,28,38,35,61,27,23,26,55,2,53,71,20,35,61,29,21,13,30,2,11,4,14,29,37,2,27,36,77,30,58,44,47,23,31,2,35,29,70,2,55,22,32,49,3,17,38,51,2,14,23,48,24,2,37,4,39,26,50,43,17,25,32,46,75,18,2,32,23,44,26,15,38,15,11,19,40,29,30,23,38,50,2,30,52,23,2,32,24,8,4,12,36,27,56,79,21,57,38,17,78,2,40,8,113,24,31,2,126,46,52,80,21,4,8,89,21,39,38,2,44,61,10,34,59,3,56,8,11,29,83,2,54,63,54,64,2,61,75,9,57,24,2,32,71,53,34,36,37,43,2,21,32,17,18,54,2,26,65,27,40,37,48,23,62,2,22,25,40,34,4,16,53,39,53,30,21,77,78,47,62,57,6,2,30,66,34,37,60,96,15,2,25,13,26,9,75,4,10,143,43,30,32,71,41,2,69,17,108,32,2,5,20,31,13,38,47,38,34,33,2,32,31,22,17,4,32,35,45,3,26,38,32,35,51,9,23,6,25,23,21,98,27,28,15,8,11,60,21,45,77,4,3,3,75,2,25,53,45,28,39,2,23,44,25,19,64,26,2,42,35,8,48,2,35,46,27,2,15,10,37,45,44,19,4,68,2,40,43,71,28,55,7,38,20,44,2,54,30,32,19,24,60,34,33,43,13,27,12,37,2,61,16,29,28,4,21,69,37,50,29,25,2,9,16,14,49,35,53,21,91,49,28,63,2,52,22,34,13,45,47,18,47,31,4,17,29,48,20,2,22,16,28,34,27,39,26,8,62,39,2,4,31,38,28,30,67,59,52,19,47,35,84,50,7,4,26,53,45,37,67,39,23,28,2,5,46,45,51,43,2,27,44,27,33,30,10,64,73,2,52,24,32,4,25,2,26,22,24,37,7,41,19,24,46,88,62,5,57,7,27,26,108,6,43,2,72,28,84,2,40,35,40,27,30,44,43,2,37,61,47,48,4,59,20,57,32,53,66,2,20,59,11,2,35,7,54,4,13,69,2,54,23,39,15,44,4,2,8,28,37,2,25,10,137,37,2,30,40,56,69,77,75,4,6,34,64,42,38,2,11,61,18,67,35,40,17,31,2,35,39,20,16,35,2,7,15,65,2,24,13,45,22,5,51,77,3,4,3,24,62,47,32,35,25,23,55,63,2,76,25,41,57,23,25,11,31,67,52,2,20,32,36,38,77,2,11,2,4,45,13,55,52,18,30,27,22,32,3,17,63,38,24,43,26,57,4,172,41,106,42,3,8,37,84,53,2,37,12,72,2,59,49,22,43,41,14,8,71,23,4,14,11,17,11,73,69,31,20,32,49,4,28,3,35,58,16,22,40,16,25,25,51,4,29,39,18,41,34,65,2,22,31,56,44,26,50,54,26,69,2,7,38,4,4,52,64,15,29,41,80,32,82,43,10,40,39,32,47,2,24,35,23,72,73,2,69,71,4,13,2,14,35,43,32,2,72,55,15,30,36,34,65,2,38,22,39,62,45,58,54,40,35,8,21,24,44,4,17,50,48,44,38,2,28,19,20,40,48,2,22,42,47,41,32,24,84,43,36,36,61,24,5,11,35,44,15,24,4,31,45,50,78,16,26,26,20,32,40,55,94,83,2,57,50,33,51,59,2,58,36,10,48,86,33,67,2,46,49,57,102,117,2,39,17,40,43,2,26,26,88,19,4,30,24,2,21,43,39,51,44,20,29,2,32,58,22,2,5,6,3,2,27,40,2,26,52,70,21,3,20,31,35,38,45,52,29,4,8,63,2,55,47,58,55,51,23,61,69,54,2,45,60,88,92,2,29,40,19,36,4,7,62,47,27,2,50,2,22,16,43,39,35,69,2,38,14,67,36,5,52,159,2,60,36,49,4,5,35,53,74,2,35,21,12,36,17,71,53,36,40,26,6,62,32,44,20,14,46,2,39,104,48,4,13,43,57,24,19,22,28,79,3,41,40,36,20,29,29,19,27,55,2,40,26,25,26,29,28,35,55,48,63,4,10,19,28,158,2,32,20,113,73,8,41,2,58,22,38,48,44,34,2,77,26,73,6,4,39,2,40,13,25,7,31,34,55,11,23,43,9,2,40,23,15,44,62,80,2,30,27,30,57,10,11,16,23,35,33,18,6,42,49,8,4,39,36,49,38,22,19,2,17,12,42,39,49,20,30,11,30,3,57,11,15,70,23,7,41,53,43,56,2,19,44,38,31,15,4,3,18,18,25,13,94,2,27,19,49,28,46,92,2,62,29,2,20,23,29,49,58,14,16,51,23,61,37,60,6,3,24,15,23,19,35,52,61,2,78,68,31,2,49,22,34,43,15,33,41,26,18,2,38,38,43,39,2,10,30,14,20,43,4,7,80,31,76,50,52,2,43,25,47,18,9,63,39,11,2,97,2,26,7,101,49,2,72,35,15,4,27,41,58,124,33,7,68,43,83,112,46,23,58,76,7,39,2,17,51,19,24,4,26,51,66,34,28,17,29,42,2,47,28,34,13,44,44,35,52,45,11,2,16,60,12,28,64,49,66,4,43,16,63,27,74,44,68,59,35,53,48,48,19,27,48,45,46,19,2,13,9,24,43,49,13,2,27,33,81,12,96,3,47,2,42,33,31,26,68,36,18,2,14,31,30,68,24,23,38,19,31,2,11,40,28,13,44,2,4,55,39,31,43,59,31,15,45,53,38,2,43,25,28,32,19,119,2,63,55,35,7,37,7,39,35,4,13,58,32,37,35,63,23,48,43,14,51,4,4,4,57,50,96,19,43,18,31,18,33,19,4,36,2,36,24,40,20,24,33,22,102,2,11,22,17,20,25,39,50,18,39,2,47,13,16,135,2,44,6,50,9,33,39,13,4,18,2,42,26,53,20,47,15,59,71,2,38,33,11,38,83,2,54,62,47,13,24,24,29,17,33,58,47,4,23,2,86,35,22,52,33,99,42,23,60,5,51,38,70,33,50,2,36,43,65,2,63,32,16,20,13,14,4,9,27,2,43,18,25,36,10,13,14,7,48,2,30,57,33,11,2,20,37,25,63,92,42,30,20,51,5,47,85,10,2,56,23,35,2,4,41,15,24,55,32,26,28,28,2,87,16,21,52,32,47,56,75,70,32,38,43,33,77,55,23,4,17,2,100,44,40,39,131,2,35,13,13,25,23,61,24,37,40,7,30,55,17,23,61,2,26,36,20,2,12,4,28,32,39,41,2,43,45,37,6,54,42,29,31,44,18,26,54,38,89,19,11,62,13,23,21,53,64,12,42,48,17,52,13,31,10,35,130,72,24,57,38,11,57,52,25,26,231,53,45,44,51,32,17,39,5,35,50,70,123,54,40,70,57,45,89,84,12,37,80,13,81,62,49,9,32,19,23,133,106,63,61,79,20,40,57,11,32,7,30,58,132,17,67,70,43,141,71,34,57,31,70,34,50,64,96,61,12,93,46,31,137,42,74,28,111,70,131,5,58,26,95,92,61,23,101,29,103,26,31,17,5,30,46,38,80,46,59,67,18,60,201,92,51,67,52,110,49,74,50,51,152,84,29,29,40,66,40,44,153,105,82,40,20,115,34,90,85,145,10,16,55,13,64,99,23,39,20,69,125,57,30,26,22,13,149,161,122,118,58,112,54,207,44,33,83,112,271,74,20,123,40,81,56,104,97,141,266,61,147,14,95,99,102,100,42,54,48,73,102,57,92,140,140,76,113,5,37,28,110,51,22,58,34,75,84,76,21,66,10,6,6,23,49,73,92,25,25,21,19,10,2,10,2,4,17,13,24,19,36,37,51,96,55,99,97,57,39,16,4,27,27,25,22,24,34,20,32,41,51,36,28,27,28,81,75,50,24,42,15,45,101,17,71,38,12,59,13,14,33,41,101,56,32,68,69,13,36,29,36,67,47,22,37,9,48,177,10,29,18,32,14,17,16,67,14,64,30,5,20,16,64,61,38,10,75,63,79,164,6,17,8,100,33,101,58,58,34,62,26,32,93,25,49,22,61,33,44,43,5,53,13,110,21,68,41,46,41,26,17,9,25,40,11,25,34,27,19,37,3,20,22,26,53,46,46,68,28,60,29,163,28,14,45,58,69,4,31,6,7,33,57,99,16,63,97,58,37,15,12,15,18,54,23,28,26,20,24,33,21,34,40,50,36,24,9,27,29,72,14,13,22,16,32,80,18,10,42,43,15,42,103,20,17,51,20,14,12,9,14,17,35,12,11,43,24,61,60,33,50,67,75,13,25,11,23,13,15,7,17,37,37,131,65,15,7,44,40,19,13,20,11,17,9,14,27,19,79,34,43,43,55,46,62,73,39,63,15,88,16,16,45,18,78,28,36,26,68,2,62,25,26,26,30,2,29,25,155,3,30,37,21,44,36,248,7,33,62,39,84,21,37,101,93,67,24,45,121,124,50,74,19,18,40,16,15,19,15,64,64,45,10,76,42,28,17,14,19,34,15,61,77,130,30,50,54,8,44,29,29,6,17,20,13,91,55,57,34,113,15,26,46,21,58,47,62,32,45,17,87,23,32,49,24,16,90,23,21,29,10,7,34,17,17,30,62,23,32,15,67,35,16,42,30,38,6,53,65,72,36,10,37,43,39,11,30,94,17,7,7,95,96,3,23,113,34,35,18,30,31,35,37,34,10,2,14,54,32,35,34,2,3,53,6,18,25,49,11,27,33,50,28,37,93,51,24,10,12,11,23,33,16,3,87,63,33,22,14,18,38,64,63,23,14,39,34,29,3,19,46,23,23,34,33,28,16,44,39,21,24,17,35,72,16,60,42,61,27,21,35,14,3,78,24,23,28,15,24,7,10,24,27,29,20,25,51,30,43,16,40,4,5,3,14,13,30,14,47,7,55,45,31,19,28,42,81,28,15,24,52,3,55,62,45,65,42,26,48,43,28,28,43,20,55,43,56,83,14,8,6,13,28,13,10,54,13,17,11,45,19,68,21,15,20,26,19,18,3,90,66,24,117,70,49,21,45,36,57,19,192,92,78,30,35,45,23,87,3,15,36,108,25,41,10,14,51,27,30,68,68,21,15,44,51,82,203,23,10,94,43,21,5,3,71,115,45,22,67,76,64,43,27,4,61,25,28,31,19,137,153,7,24,69,23,66,42,22,33,58,25,14,11,51,59,12,47,25,32,62,23,32,12,68,35,16,43,32,40,6,59,89,72,11,40,42,40,10,36,19,75,17,7,69,95,4,7,73,32,21,28,67,36,34,26,73,12,65,21,104,18,17,5,3,17,30,39,17,18,37,31,46,31,49,18,45,50,28,10,40,73,20,37,23,52,65,33,23,33,38,65,60,18,3,18,28,147,43,114,9,28,23,29,15,34,7,15,7,10,26,26,27,21,26,49,12,22,21,14,13,30,12,58,29,34,41,15,9,5,3,118,44,54,45,31,19,27,40,81,28,17,23,11,93,65,45,62,42,26,47,43,28,28,42,22,29,3,39,90,81,14,8,6,14,32,75,13,17,11,45,19,21,8,44,42,68,23,114,43,27,50,20,46,36,9,3,67,190,92,21,48,10,30,35,21,16,4,57,106,24,40,7,14,53,25,31,70,64,25,7,3,15,22,48,51,82,205,23,10,101,24,18,4,42,71,119,45,42,67,44,31,64,26,3,35,66,26,27,31,19,92,46,150,2,78,6,60,29,74,82,101,68,27,39,43,72,7,3,98,2,109,3,62,22,100,70,53,18,20,41,154,2,71,51,21,223,3,83,6,27,112,5,78,33,7,31,11,32,17,44,31,18,12,19,24,5,2,2,7,45,13,19,10,29,32,28,62,22,53,29,10,99,49,33,38,32,32,30,12,37,42,17,80,121,2,4,18,22,52,11,7,2,2,22,38,46,110,83,52,35,26,79,59,38,2,11,55,43,20,123,22,80,40,47,38,5,7,2,2,5,29,29,101,67,96,89,13,95,41,85,11,45,14,41,32,19,48,29,76,42,2,2,12,53,14,114,32,20,2,18,2,20,2,20,73,19,31,54,32,9,7,7,10,156,13,29,35,10,11,2,9,2,32,19,29,34,35,8,6,11,9,56,75,22,74,49,46,26,18,14,14,10,16,23,15,22,22,21,68,91,13,54,31,15,6,21,22,101,31,2,2,2,5,5,20,24,14,30,51,11,6,57,20,14,15,4,8,21,38,35,53,4,95,2,41,33,70,42,23,6,44,54,35,21,5,2,2,2,2,4,30,77,70,46,34,42,14,16,129,31,36,45,35,26,4,28,71,2,85,14,14,42,44,21,61,44,71,16,22,28,10,2,2,2,5,50,65,29,45,59,50,37,6,32,25,77,25,12,92,48,31,17,62,15,18,22,50,45,43,64,41,9,2,2,2,5,41,88,14,40,41,41,23,11,18,9,15,64,87,17,38,21,7,75,72,26,31,36,48,7,2,2,2,4,2,23,67,50,25,32,48,35,88,83,10,7,3,12,6,17,23,58,9,71,17,23,14,38,18,59,25,39,80,24,59,34,8,2,2,2,5,92,132,87,3,67,121,6,28,106,60,9,17,30,44,70,16,165,2,2,2,2,3,2,3,108,38,40,61,40,84,26,14,23,9,52,6,3,15,17,2,2,2,3,3,39,52,74,85,61,58,42,15,90,6,70,46,61,39,55,39,32,33,73,101,18,7,2,2,2,3,3,60,56,76,50,62,59,21,39,100,38,4,156,21,98,32,77,34,26,27,74,25,11,9,6,9,2,2,2,2,2,3,11,58,23,19,65,49,47,29,64,93,31,74,8,63,21,31,31,53,39,30,34,59,28,136,17,7,3,2,3,3,29,118,40,12,60,41,14,73,63,28,5,4,27,40,33,43,48,25,89,86,54,81,21,2,2,2,3,3,33,38,89,141,2,10,87,14,48,4,21,90,23,39,9,2,9,71,80,138,18,2,2,2,2,2,2,2,23,38,95,52,77,15,67,10,32,52,30,4,68,14,26,75,23,36,24,42,173,21,44,16,2,2,2,2,2,2,2,33,47,39,29,34,36,10,82,64,23,26,41,14,43,17,4,32,76,145,19,21,31,94,39,38,87,2,2,2,3,3,62,14,45,25,35,54,51,68,34,62,54,73,37,29,72,71,30,37,8,15,35,50,42,19,2,2,2,3,3,27,32,120,39,41,31,21,63,44,50,30,48,110,32,33,49,54,33,20,52,28,33,47,11,2,2,2,2,2,2,2,9,36,25,95,50,39,102,73,33,59,41,40,34,34,80,72,18,63,89,64,8,2,2,2,3,3,13,28,74,14,29,49,59,62,77,62,21,40,18,36,23,30,49,64,29,68,48,23,76,92,8,59,6,25,2,2,2,3,3,63,11,13,16,20,23,65,42,31,30,22,41,49,49,44,38,36,4,64,35,61,19,33,7,37,59,37,31,47,33,29,2,2,2,3,3,43,24,74,21,40,53,190,13,81,5,2,2,2,3,3,2,56,31,15,89,8,48,3,9,54,15,26,101,46,6,2,29,24,81,11,33,23,34,54,179,74,43,5,2,2,2,3,3,2,10,35,133,39,36,40,56,21,37,30,91,2,84,11,14,40,44,51,32,73,66,71,16,18,2,2,2,3,3,2,33,37,71,64,3,8,69,22,27,69,50,2,13,44,59,74,81,18,140,18,12,2,2,2,3,2,2,2,11,88,53,84,3,22,122,47,24,5,17,4,2,12,3,40,43,80,84,52,65,99,6,33,7,2,2,2,3,3,2,112,3,19,16,112,48,33,48,37,30,55,10,2,45,8,26,49,36,26,42,16,67,16,76,80,37,2,2,2,3,3,2,59,8,17,58,52,140,93,52,16,22,4,2,161,18,73,154,108,22,2,2,2,3,3,2,31,135,8,109,66,27,92,41,12,2,81,230,56,10,75,12,39,20,13,2,2,2,3,3,2,78,39,199,15,41,32,102,13,15,2,12,18,59,65,43,43,53,48,77,53,48,7,2,2,2,2,2,2,2,2,58,41,50,63,67,36,48,48,58,31,10,2,18,42,96,8,54,52,19,19,23,33,129,21,5,2,2,2,3,3,2,33,27,43,13,30,22,10,17,58,40,191,26,6,4,2,45,40,150,182,6,69,2,2,2,3,3,2,33,26,55,14,44,101,31,59,29,54,43,2,2,2,3,3,45,10,46,20,47,38,57,101,50,36,55,28,98,19,93,28,59,104,57,61,8,26,11,2,2,2,3,91,86,92,34,35,61,27,33,18,65,46,14,2,2,2,3,2,2,51,46,53,44,26,8,15,30,31,26,38,94,105,83,54,48,22,68,30,32,37,31,51,50,46,49,5,2,2,2,3,2,2,68,112,46,30,40,72,15,26,85,47,10,26,28,49,85,40,20,47,31,18,46,60,11,113,5,2,2,2,3,2,2,59,57,58,96,39,45,24,23,124,20,96,37,24,22,24,64,42,53,34,13,67,21,73,88,59,78,75,88,5,2,2,2,3,2,2,22,36,35,88,39,57,39,21,91,41,23,12,73,16,40,62,30,51,42,22,18,41,68,40,16,26,27,14,2,2,2,3,2,2,64,42,41,45,98,18,77,51,43,86,4,54,61,33,88,21,86,87,46,17,2,2,2,3,2,2,29,37,61,55,41,63,45,39,89,15,58,6,13,14,4,5,8,6,29,92,15,25,11,23,3,2,7,3,4,2,16,7,12,56,9,2,2,2,3,2,2,119,3,78,95,129,3,33,36,67,13,39,163,12,42,5,18,29,31,52,63,9,26,34,13,2,2,2,3,2,2,51,45,79,15,21,28,48,48,13,99,35,32,31,45,21,16,46,58,36,26,18,36,7,73,39,64,100,27,5,5,2,2,2,3,2,2,3,60,56,31,15,55,78,88,64,19,10,24,58,46,68,8,27,51,31,66,68,33,15,71,21,19,2,2,2,3,2,2,49,40,26,46,11,53,51,35,55,2,62,57,28,118,8,13,149,58,47,100,56,98,8,58,2,2,2,3,2,2,34,37,163,84,10,52,103,75,65,20,8,47,56,39,27,62,44,45,3,24,15,19,7,2,2,2,3,2,2,59,72,81,45,40,59,42,6,79,42,22,10,38,4,22,65,58,37,49,59,85,33,9,21,102,25,2,2,2,3,2,2,3,47,64,177,76,32,102,89,7,8,3,8,82,73,46,23,25,9,23,21,43,34,14,44,50,17,51,26,23,5,2,2,2,3,2,2,33,43,9,48,40,46,42,67,53,24,42,28,36,65,29,20,11,74,48,67,44,11,48,89,18,72,6,16,15,23,2,2,2,3,2,2,46,12,35,60,45,44,29,120,30,22,21,30,53,29,6,23,4,112,16,13,19,15,62,103,16,50,26,13,50,25,24,2,2,2,3,2,2,16,35,3,40,104,56,9,29,9,37,56,19,12,8,101,84,67,9,22,55,16,57,120,13,2,2,2,3,2,2,15,58,22,22,79,64,80,19,8,75,14,42,57,12,9,39,23,51,35,38,38,23,43,52,59,50,68,32,2,2,2,3,2,2,38,84,59,30,105,82,69,53,15,72,109,40,30,66,80,16,7,27,37,2,2,2,3,2,2,63,54,86,80,113,17,46,62,51,107,21,35,40,34,17,33,30,12,78,59,49,9,2,2,2,3,2,2,40,81,43,13,50,21,13,45,39,42,31,37,28,17,41,24,16,18,6,28,44,97,35,34,36,35,42,14,48,49,56,54,7,2,2,2,3,2,2,27,30,28,14,114,42,47,75,41,81,31,33,11,6,26,40,42,31,21,73,26,20,59,64,16,61,58,41,41,6,2,2,2,3,2,2,66,24,36,112,4,16,32,79,57,14,21,9,4,28,78,53,56,62,15,41,65,29,53,27,41,42,9,30,14,2,2,2,3,2,2,66,63,51,20,68,26,28,20,33,114,38,20,4,67,48,8,59,78,32,34,9,38,82,41,14,7,2,2,2,3,2,2,74,33,52,56,44,61,95,37,7,37,74,8,50,53,9,3,20,2,20,2,118,45,70,21,29,25,22,30,14,2,2,2,3,2,2,45,42,58,57,58,25,4,20,20,3,29,30,10,12,8,25,3,23,22,7,72,11,6,18,117,43,154,16,37,59,64,67,2,2,2,3,2,2,18,82,60,153,57,41,27,52,61,16,69,82,121,18,71,33,51,46,26,2,2,2,3,2,2,31,50,27,43,26,166,18,63,50,38,30,12,9,56,26,67,3,22,63,61,24,84,54,9,2,2,2,3,2,2,24,113,21,117,28,44,92,23,14,25,30,4,33,129,33,47,32,73,44,40,19,11,17,5,2,2,2,3,2,2,94,9,21,31,82,92,73,44,24,50,9,6,32,33,8,128,71,49,40,52,36,11,15,51,63,20,32,5,2,2,2,3,2,2,46,51,28,45,169,42,51,98,57,27,79,46,40,35,63,33,22,47,47,42,35,29,14,19,2,2,2,3,2,2,60,105,34,65,52,24,35,20,45,72,18,8,27,108,34,74,49,83,79,79,15,57,10,2,2,2,3,2,2,5,18,183,47,74,28,68,31,54,22,26,4,188,33,25,14,39,32,12,5,3,9,25,10,21,21,31,35,20,13,37,48,13,36,78,114,36,34,104,100,19,11,12,24,43,20,16,2,2,2,3,2,93,65,105,60,96,19,43,49,29,52,40,57,32,31,22,20,52,46,31,46,59,139,37,7,2,2,2,3,2,71,61,137,38,23,36,73,4,22,22,20,53,25,11,62,95,134,157,61,32,44,7,2,2,2,2,2,2,66,64,50,96,54,91,56,35,15,2,9,11,23,145,63,12,46,3,49,42,34,18,30,24,29,30,29,7,2,2,2,3,2,31,57,73,122,58,11,6,25,83,25,19,12,46,25,32,58,3,6,8,34,56,120,98,118,11,63,47,30,7,2,2,2,3,2,35,78,26,67,29,40,48,128,47,34,29,4,47,78,115,51,31,74,22,7,93,19,6,16,66,5,2,2,2,2,2,2,49,40,36,52,39,66,70,87,33,36,29,6,9,6,4,64,66,29,62,39,29,55,55,61,26,66,10,8,20,2,2,2,3,2,14,44,98,51,14,32,28,90,56,32,30,49,26,4,16,79,32,21,73,16,79,116,35,34,13,48,29,103,93,60,66,106,61,39,2,2,2,3,2,2,25,66,43,37,57,32,31,22,18,50,78,44,58,145,40,23,36,56,96,110,25,80,9,2,2,2,3,2,2,112,61,31,44,82,68,129,4,27,37,12,48,51,43,34,35,57,75,98,17,53,5,2,2,2,3,2,2,19,213,34,143,18,51,24,174,119,49,208,5,2,2,2,3,2,2,10,39,36,50,45,66,70,88,32,35,52,6,9,6,15,40,64,31,61,38,29,56,55,92,44,11,8,13,9,2,2,2,3,2,2,7,25,72,46,98,40,15,33,28,90,56,32,30,60,79,33,81,23,17,40,56,55,12,36,3,35,64,53,27,24,63,22,173,50,15,66,12,20,20,31,6,9,2,2,2,3,2,110,21,30,117,56,105,113,33,9,32,2,4,128,8,34,91,27,19,16,22,32,3,4,31,55,26,46,18,2,2,2,3,2,17,42,16,31,28,46,34,56,31,97,41,94,36,17,6,29,24,76,18,51,78,48,27,40,21,40,31,6,7,2,2,2,3,2,37,48,33,101,29,21,73,40,126,15,18,5,44,94,28,10,14,58,101,40,148,31,2,2,2,3,2,10,24,14,60,42,32,120,32,60,80,24,13,28,59,44,76,13,5,37,19,71,19,124,27,81,2,2,2,3,2,18,14,70,99,74,177,83,46,64,39,104,38,75,62,26,49,12,2,2,2,2,2,2,21,42,90,17,54,134,48,33,74,47,30,110,43,52,46,112,53,41,31,41,27,5,2,2,2,3,2,53,188,50,45,70,33,69,8,70,79,57,27,59,45,79,28,84,7,2,2,2,2,2,2,25,37,26,47,174,32,137,23,20,44,9,5,8,9,10,8,53,8,26,37,31,65,19,29,30,39,19,19,13,70,18,47,16,116,123,12,111,31,110,27,12,9,2,2,2,2,8,9,37,21,39,33,20,30,19,72,138,155,12,5,49,23,65,135,103,56,25,7,2,2,2,2,29,47,34,24,15,27,43,27,31,44,42,52,45,51,24,84,100,41,9,15,31,51,57,86,20,12,12,5,2,2,2,2,11,56,50,17,24,25,28,109,3,38,43,9,53,11,84,17,153,34,34,35,52,28,13,8,34,30,2,2,2,2,34,78,59,30,39,42,38,11,25,147,101,56,12,23,47,14,25,39,80,22,65,53,26,58,14,47,114,130,67,25,97,5,2,2,2,2,27,69,84,14,24,64,50,50,31,64,45,70,4,62,53,114,69,45,50,53,160,2,2,2,2,33,126,32,36,91,43,81,31,17,11,3,43,13,17,31,64,25,18,18,55,28,73,36,18,112,48,2,2,2,2,24,38,22,20,22,57,71,78,91,65,82,38,32,65,13,21,41,42,113,49,50,36,24,47,7,2,2,2,2,9,28,96,40,25,14,24,30,30,70,61,36,59,30,17,20,71,30,63,41,13,38,70,46,14,23,22,12,15,30,22,39,5,2,2,2,2,107,157,117,32,97,66,10,51,52,16,82,94,44,49,12,40,41,16,2,2,2,2,52,100,89,23,9,21,107,21,17,8,26,26,9,13,4,23,55,26,47,129,23,18,36,52,57,21,19,41,28,5,2,2,2,2,28,96,134,41,42,31,50,61,58,26,56,9,2,2,2,2,19,10,71,20,100,12,12,36,49,41,16,110,25,11,11,16,25,15,10,44,17,16,12,63,21,14,2,2,2,2,2,2,9,35,19,14,77,64,43,88,75,52,46,52,17,22,22,50,80,30,7,83,115,20,35,7,2,2,2,2,33,103,37,48,120,34,77,41,24,14,4,25,23,30,74,129,78,26,30,23,4,10,20,22,5,2,2,2,2,23,22,58,12,28,33,26,19,62,50,45,21,35,50,35,15,9,6,29,123,92,5,19,14,12,57,4,46,53,2,2,2,2,8,18,36,7,67,35,10,11,26,66,27,50,29,27,19,94,24,22,21,35,144,33,44,34,29,42,56,57,31,9,7,2,2,2,2,26,140,14,73,103,27,57,42,10,45,50,7,4,45,41,21,41,50,17,41,26,99,47,69,92,7,2,2,2,2,57,37,100,102,6,21,6,14,60,20,57,11,37,39,75,15,47,41,59,119,38,98,68,2,2,2,2,50,59,75,35,61,68,44,143,47,14,81,77,40,10,10,31,56,24,40,37,29,12,2,2,2,2,53,50,58,26,61,25,45,66,142,12,4,24,43,25,40,61,68,31,135,52,73,14,2,2,2,2,81,16,53,109,100,30,51,47,19,42,19,127,78,28,44,22,23,55,39,42,52,63,18,2,2,2,2,52,117,62,119,14,121,45,30,7,52,108,98,35,47,73,107,25,23,103,89,71,64,64,54,34,4,44,59,41,17,74,58,103,40,102,9,2,2,2,2,2,2,2,129,42,61,144,102,83,59,28,46,158,54,46,18,11,13,22,62,31,2,2,2,2,2,40,58,34,55,89,58,101,92,31,48,35,157,70,17,80,35,18,53,2,2,2,2,2,13,22,24,35,78,40,18,50,64,98,50,28,26,51,109,166,43,26,10,109,2,2,2,2,2,2,2,6,11,2,9,2,9,29,46,10,39,98,39,46,20,47,23,26,12,62,4,16,19,50,43,31,31,38,42,39,49,29,30,53,68,3,61,7,2,2,2,2,2,67,40,26,70,23,85,57,32,109,42,15,15,7,4,20,56,115,9,18,19,65,46,9,2,2,2,2,2,52,21,89,96,61,43,37,20,32,54,35,22,14,10,17,13,2,2,2,2,2,2,2,11,60,48,72,55,68,138,51,48,130,71,66,82,17,41,20,15,27,51,13,9,2,2,2,2,2,2,2,22,112,67,109,84,48,65,19,10,92,103,151,36,151,24,32,14,5,2,2,2,2,2,2,2,42,89,31,39,94,61,130,46,118,6,139,67,128,2,5,19,58,60,7,2,2,2,2,2,2,2,51,14,41,25,76,57,58,81,192,16,36,50,52,60,32,30,155,64,53,56,2,2,2,2,2,2,2,158,69,50,109,98,136,91,9,37,42,64,79,17,43,2,2,2,2,2,2,2,33,121,13,20,132,41,63,76,96,40,115,56,80,18,43,56,31,2,2,2,2,2,2,2,7,138,86,59,63,43,91,2,2,2,2,2,2,2,20,113,24,47,109,95,13,74,30,74,78,42,48,86,27,49,53,75,7,24,10,2,2,2,2,2,2,2,63,12,26,40,33,50,26,26,35,47,45,186,43,52,18,87,18,47,25,24,21,119,119,7,2,2,2,2,2,2,2,42,2,13,2,26,59,58,45,21,78,71,13,6,12,12,52,38,12,25,96,67,51,79,15,43,52,8,2,2,2,2,2,2,2,33,51,43,16,11,99,74,64,11,21,50,17,12,17,22,12,62,31,19,108,18,14,2,8,2,23,112,30,10,2,2,2,2,2,2,2,19,124,45,51,62,34,29,78,37,48,8,41,56,37,45,51,34,55,41,17,36,48,23,54,115,13,74,18,9,35,9,36,63,30,36,24,57,42,28,37,29,5,2,2,2,2,2,2,2,55,26,28,8,63,36,33,45,35,27,50,68,37,7,5,45,59,18,76,69,24,22,36,45,41,5,2,2,2,2,2,2,2,4,45,40,75,32,33,35,6,44,7,46,121,35,17,37,11,103,45,26,22,53,58,34,42,133,12,12,43,14,39,13,11,11,7,72,89,48,31,2,2,2,2,2,2,2,8,61,33,18,8,57,11,52,19,11,54,17,39,46,83,2,105,52,58,135,27,11,2,2,2,2,2,2,2,37,49,37,43,10,7,38,70,75,57,51,18,83,67,41,35,32,26,34,38,16,37,50,50,48,22,40,72,38,102,53,51,22,26,80,5,2,2,2,2,2,2,2,9,76,50,75,51,75,64,114,2,44,39,52,63,86,39,38,7,2,2,2,2,2,2,2,8,27,66,88,81,18,14,71,60,58,61,28,24,30,17,14,36,5,64,50,8,23,13,62,34,123,49,59,23,12,21,18,10,5,2,2,2,2,2,2,2,59,26,8,56,38,37,84,60,47,61,29,7,95,65,52,36,42,5,33,58,38,34,2,2,2,2,2,2,2,16,44,78,15,19,14,78,21,59,26,15,49,40,34,9,62,17,17,114,53,75,89,147,17,112,163,33,108,15,62,57,7,2,2,2,2,2,2,2,6,58,37,21,44,75,32,92,71,22,70,5,114,39,14,67,37,43,43,147,36,4,2,2,2,2,2,2,2,163,174,91,64,33,19,4,107,94,7,22,74,183,27,10,2,2,2,2,2,2,2,42,13,53,13,52,37,112,25,69,37,34,41,81,45,43,90,25,26,58,106,17,14,2,2,2,2,2,2,2,37,96,119,64,20,80,19,32,72,30,81,136,49,31,36,50,35,125,14,2,2,2,2,2,2,2,6,43,148,30,104,31,46,34,136,21,60,14,55,61,62,35,9,59,13,36,49,11,2,2,2,2,2,2,2,22,67,55,72,72,5,62,41,32,42,31,66,21,19,20,40,14,22,11,62,17,67,18,32,42,62,31,20,2,2,2,2,2,2,2,26,19,37,52,21,5,31,35,40,42,33,30,80,40,40,9,22,16,6,8,28,68,7,34,61,52,43,52,92,40,5,2,2,2,2,2,2,2,53,39,45,29,65,33,20,35,34,34,69,103,10,31,60,33,33,56,161,57,17,40,60,2,2,2,2,2,2,2,5,33,110,84,19,46,66,34,81,15,12,41,100,39,25,9,64,31,54,135,58,2,2,2,2,2,2,2,42,54,15,35,111,39,55,44,56,72,27,11,94,24,15,13,62,36,49,44,59,53,50,22,28,39,2,2,2,2,2,2,2,71,51,49,23,83,27,68,37,69,56,44,4,59,86,53,8,18,33,25,27,7,47,37,132,46,14,2,2,2,2,2,2,2,23,120,70,41,7,139,30,89,61,16,14,81,55,13,26,36,10,76,39,2,2,2,2,2,2,2,29,102,46,35,53,136,28,42,28,20,60,42,60,69,72,50,20,25,42,86,40,82,7,2,2,2,2,2,2,2,18,27,163,46,98,102,10,10,19,58,53,77,62,66,25,52,61,13,2,2,2,2,2,2,2,31,37,87,30,52,91,18,31,60,48,34,31,19,58,15,18,296,14,23,81,61,21,7,2,2,2,2,2,2,2,26,79,2,17,26,21,21,85,55,30,46,75,13,30,9,39,24,19,157,33,40,148,74,21,36,2,2,2,2,2,2,2,69,35,31,41,59,111,25,77,19,90,55,42,139,38,79,114,2,2,2,2,2,2,2,21,43,21,143,2,20,24,28,65,73,28,4,42,38,113,61,49,4,20,40,16,127,51,14,2,2,2,2,2,2,2,20,85,41,38,50,75,169,38,28,38,22,16,48,51,31,31,69,46,23,19,16,81,37,34,16,8,2,2,2,2,2,2,2,35,68,28,79,46,24,80,35,71,44,21,39,11,6,10,57,64,114,98,41,18,33,65,11,32,14,2,2,2,2,2,2,2,17,163,65,21,70,20,60,29,25,75,93,34,56,82,70,50,118,7,38,2,2,2,2,2,2,2,8,18,14,30,96,61,43,25,18,46,44,41,44,36,7,11,17,193,129,74,100,37,2,2,2,2,2,2,2,24,77,29,63,47,52,63,31,23,66,12,44,18,17,34,16,32,34,17,8,21,51,20,34,58,17,22,189,5,2,2,2,2,2,2,2,44,49,64,22,120,46,180,5,29,19,4,70,65,14,14,8,119,75,78,39,61,14,2,2,2,2,2,2,2,39,52,45,54,64,28,30,18,29,100,32,13,14,142,77,71,275,2,2,2,2,2,2,2,32,136,64,68,18,154,55,15,30,44,58,60,42,84,10,47,121,17,2,2,2,2,2,2,2,85,27,43,86,78,44,16,51,34,51,42,30,47,56,87,50,5,2,2,2,2,2,2,2,70,24,44,15,35,74,74,85,36,97,16,54,148,78,51,11,15,51,42,47,2,2,2,2,2,2,2,13,19,73,66,30,46,144,91,46,5,26,55,28,101,16,38,21,39,55,28,75,7,23,2,2,2,2,2,2,2,65,61,49,61,37,176,16,48,34,13,70,58,39,54,40,76,46,45,159,8,2,2,2,2,2,2,2,135,38,87,66,45,17,100,73,8,67,92,41,114,21,156,11,2,2,2,2,2,2,2,96,24,20,18,38,11,20,39,110,26,47,19,108,4,3,43,69,122,12,8,42,51,91,98,53,2,2,2,2,2,2,2,72,76,37,37,30,99,68,47,24,30,54,23,23,48,17,93,21,16,50,45,47,29,94,2,2,2,2,2,2,2,10,57,44,78,56,49,49,22,42,48,28,55,30,126,32,28,27,15,34,43,11,25,43,33,28,34,46,60,2,2,2,2,2,2,2,15,49,24,63,36,42,68,25,52,40,17,26,23,58,30,63,26,20,10,24,16,13,44,3,69,45,12,17,50,27,53,43,37,20,7,2,2,2,2,2,2,2,8,54,26,50,11,28,84,36,29,105,42,23,62,175,17,36,90,52,74,59,5,2,2,2,2,2,2,2,105,26,20,18,26,43,56,49,63,44,60,29,21,83,42,28,79,33,74,10,46,57,41,57,28,5,2,2,2,2,2,2,2,12,6,63,30,30,69,81,30,23,41,63,63,22,73,55,48,34,14,23,82,151,52,57,2,2,2,2,2,2,2,21,41,60,34,33,31,26,41,69,45,29,53,41,104,19,81,39,39,35,30,22,71,35,43,6,24,2,2,2,2,2,2,2,105,31,97,35,78,31,35,47,14,23,20,11,254,79,23,13,151,2,2,2,2,2,2,2,89,86,11,32,28,17,20,96,50,44,33,34,109,119,21,29,36,91,10,15,24,36,2,2,2,2,2,2,2,21,36,24,106,112,81,58,61,19,9,44,54,173,37,115,70,39,19,2,2,2,2,2,2,2,27,56,101,49,61,43,13,10,16,18,47,52,53,21,45,88,108,83,15,34,46,23,2,2,2,2,2,2,2,33,30,48,39,38,70,50,26,15,87,97,4,32,17,31,44,48,92,59,87,44,27,31,44,15,2,2,2,2,2,2,2,7,107,92,70,38,78,39,47,37,35,24,54,62,34,49,59,17,83,17,169,2,2,2,2,2,2,2,27,48,50,92,49,165,27,31,53,4,115,38,67,19,9,23,26,142,36,47,93,52,56,70,46,99,128,103,12,28,12,2,2,2,2,2,2,2,6,37,38,39,18,112,30,54,101,12,72,27,21,27,147,75,40,13,26,61,48,45,2,2,2,2,2,2,2,12,66,14,37,9,67,65,131,50,43,71,29,25,67,32,43,38,37,96,34,26,7,2,2,2,2,2,2,2,5,48,65,51,52,126,33,46,52,80,36,146,41,112,17,102,17,29,16,40,6,7,2,2,2,2,2,2,2,14,58,22,49,50,64,83,16,66,17,29,39,68,83,28,203,60,45,10,7,2,2,2,2,2,2,2,19,56,13,27,120,37,114,61,27,28,34,36,93,72,235,21,2,2,2,2,2,2,2,86,37,35,43,33,10,16,37,22,36,54,15,13,70,4,41,113,41,32,16,73,41,67,38,2,2,2,2,2,2,2,230,80,83,94,16,91,23,60,24,90,22,27,72,94,2,2,2,2,2,2,2,43,31,52,92,86,7,81,90,44,6,122,93,54,36,68,56,39,51,39,2,2,2,2,2,2,2,38,108,170,130,32,26,25,68,36,17,90,29,133,8,145,33,2,2,2,2,2,2,2,59,83,55,62,52,47,42,62,49,38,105,49,123,117,17,20,48,22,39,11,2,2,2,2,2,2,2,84,87,37,65,52,155,16,28,45,12,6,36,18,28,84,242,15,38,57,2,2,2,2,2,2,2,39,58,64,47,53,37,50,14,10,109,21,18,68,20,63,150,85,39,50,60,32,18,11,5,2,2,2,2,2,2,2,54,78,62,40,118,75,88,31,8,35,68,125,67,42,3,23,29,21,6,43,78,2,2,2,2,2,2,2,43,36,11,70,28,19,48,89,40,48,76,20,10,16,33,52,24,95,29,61,53,23,17,15,4,126,28,5,2,2,2,2,2,2,2,19,69,33,15,34,116,63,172,23,87,32,142,120,19,64,35,13,2,2,2,2,2,2,2,94,22,125,152,72,145,71,98,80,29,163,26,9,98,51,60,73,21,23,37,27,48,78,59,69,13,103,47,12,78,11,2,3,69,13,18,32,53,51,34,65,59,85,90,31,37,16,30,14,35,38,28,20,32,29,84,33,25,12,2,3,19,43,10,9,52,84,189,50,14,47,39,9,51,25,37,103,51,40,37,40,22,86,26,16,5,2,2,2,56,31,68,23,23,34,42,31,35,38,33,49,32,32,94,43,118,52,36,108,142,45,7,2,2,2,51,74,108,41,14,19,20,65,26,85,26,49,31,8,198,12,106,2,29,20,109,3,9,3,26,21,9,2,3,20,35,39,15,76,25,35,64,44,30,127,34,70,131,28,96,41,8,42,28,55,15,11,13,17,17,2,3,7,42,42,44,56,16,2,3,15,34,28,53,68,15,74,66,26,34,2,21,17,11,34,39,82,62,53,68,52,54,2,2,2,60,19,27,47,62,30,21,111,24,38,43,23,45,34,10,49,60,16,34,34,54,21,58,37,74,2,3,18,22,38,45,42,82,91,103,32,102,68,15,27,31,35,22,64,71,12,41,21,46,11,33,26,16,5,2,3,29,55,23,73,17,29,30,23,13,36,65,71,105,63,53,51,24,17,37,48,60,53,27,41,16,28,46,23,2,3,48,78,150,47,77,84,52,31,51,31,17,45,61,33,100,65,31,59,2,2,2,8,77,95,39,56,54,51,96,21,21,68,58,75,97,32,82,45,38,86,47,25,2,2,2,75,13,32,33,27,48,89,94,78,16,15,31,18,40,45,65,49,27,53,111,45,25,5,30,24,2,3,43,132,27,22,16,17,45,198,21,18,75,49,25,69,53,108,68,40,44,2,2,2,7,19,45,82,31,9,24,27,26,61,109,25,67,66,69,74,125,47,39,24,126,2,3,32,82,23,53,68,39,23,50,40,14,79,54,157,98,68,50,13,23,30,2,2,2,26,58,95,61,32,62,33,65,61,82,79,67,27,44,68,88,47,22,27,2,2,2,16,38,55,33,41,39,47,32,74,32,6,54,82,10,38,12,13,12,21,6,62,35,30,52,116,24,79,29,72,28,2,3,39,149,127,35,17,59,28,48,28,26,10,61,41,15,58,22,30,25,30,73,36,31,39,35,7,2,3,68,71,114,88,51,73,115,129,44,46,19,35,80,50,22,91,12,5,2,2,2,31,56,74,43,69,47,49,17,24,23,29,65,34,73,25,43,16,33,144,19,30,94,42,88,12,5,2,2,2,21,44,40,22,183,47,67,93,32,17,22,26,42,56,86,13,55,16,29,46,83,60,50,22,14,22,25,24,33,25,16,34,34,56,21,55,21,53,45,39,73,19,52,7,25,15,5,2,2,2,97,114,59,42,9,122,36,26,31,52,41,67,31,95,53,8,79,74,66,16,2,2,2,27,190,58,6,46,28,71,80,27,24,5,3,5,2,5,7,90,33,7,40,21,32,38,46,41,42,32,19,12,2,2,2,35,63,118,54,93,4,19,87,24,50,92,33,9,22,32,90,104,56,79,7,2,2,2,82,60,42,80,35,24,19,65,40,36,137,25,101,80,12,35,24,31,88,41,34,2,2,2,119,53,58,60,14,79,94,54,32,17,109,26,22,70,74,92,115,2,2,2,75,68,191,92,77,79,23,143,104,88,26,9,19,24,9,2,2,2,43,22,28,20,19,13,46,13,30,16,54,23,32,30,61,51,17,16,58,33,22,22,19,22,26,24,5,17,35,68,142,37,12,5,56,14,5,2,2,2,19,85,41,45,103,51,34,88,58,40,11,35,77,123,42,40,54,45,24,63,37,25,30,7,2,2,2,15,38,65,43,23,8,20,52,29,98,23,53,33,19,40,20,66,73,30,21,15,2,66,121,56,63,47,71,152,54,27,28,83,81,18,51,24,99,41,11,2,26,41,127,11,45,68,28,71,38,42,48,42,29,38,28,63,33,23,24,62,43,20,29,46,18,2,144,29,44,18,27,67,39,56,45,47,92,156,23,17,5,7,2,6,22,43,111,65,2,34,52,69,112,39,122,13,2,15,2,26,52,9,6,7,3,10,15,37,31,14,3,2,4,2,4,2,4,83,21,28,26,34,37,38,33,109,13,2,100,20,79,40,49,93,86,46,45,65,35,29,76,38,51,52,90,32,30,37,23,2,24,18,38,61,38,12,34,2,29,18,148,5,19,21,47,26,33,20,14,2,25,42,40,2,33,8,87,32,96,32,20,11,43,7,31,10,6,2,29,10,4,2,41,34,31,4,2,45,39,2,1,89,2,7,57,25,175,15,86,214,1,198,61,99,5,7,161,43,44,34,105,3,3,30,69,29,38,50,32,8,6,11,9,16,6,20,30,181,41,63,18,80,19,35,30,19,13,11,8,29,24,11,17,4,19,19,20,52,70,22,54,15,3,7,4,15,16,13,17,27,55,34,98,73,34,38,25,172,23,28,35,8,14,40,157,16,39,17,65,41,121,88,11,37,28,31,82,58,38,40,81,20,20,6,46,51,44,45,104,23,25,105,9,74,38,6,33,16,47,28,13,40,4,23,48,47,13,55,44,16,6,29,33,6,26,53,57,4,22,36,68,22,35,35,71,6,61,38,40,6,62,51,48,25,25,25,59,41,50,107,62,55,16,180,67,17,69,26,77,55,27,47,13,57,44,37,21,38,32,77,92,59,54,29,17,37,40,22,9,58,45,75,7,104,122,31,71,82,84,70,117,87,40,4,68,29,73,6,80,97,27,25,22,27,66,18,34,29,83,38,47,44,56,37,20,93,87,67,26,6,79,64,67,44,6,52,40,46,30,35,24,9,98,103,13,68,74,33,5,40,21,12,28,68,37,29,7,19,9,65,5,25,42,64,32,30,22,11,31,6,47,81,56,44,14,5,70,38,22,61,22,50,47,58,13,19,38,11,34,45,32,46,17,6,9,7,55,120,11,88,8,107,52,25,10,22,42,30,7,21,26,27,40,38,69,5,31,45,48,26,4,16,67,48,54,33,14,44,29,8,47,27,67,72,19,59,28,35,83,28,38,10,15,4,10,13,24,15,35,28,39,67,30,92,42,24,79,79,37,57,15,95,33,73,16,67,66,38,59,4,30,24,34,95,106,50,41,3,47,65,75,42,88,77,85,32,44,23,86,41,79,86,42,112,41,27,70,22,38,28,74,36,85,46,4,39,28,62,49,46,26,33,27,25,9,29,4,26,16,18,37,8,49,39,18,37,16,36,35,45,53,37,42,34,33,52,5,11,58,8,24,24,15,28,54,38,27,15,19,18,11,18,26,55,28,37,36,61,20,29,40,67,3,32,35,5,35,11,3,69,56,42,22,21,47,7,97,40,64,113,44,47,54,43,16,34,61,10,59,70,25,32,70,26,8,17,59,59,13,14,6,23,23,37,46,43,15,15,11,21,39,50,67,18,102,47,33,50,37,8,60,26,65,47,47,57,8,42,25,27,28,8,22,48,31,40,13,19,72,71,21,5,15,15,12,8,58,83,6,6,27,74,7,23,53,60,34,5,50,5,49,32,50,31,51,6,34,41,33,6,15,8,26,5,34,49,30,61,44,43,53,29,29,25,40,41,3,13,29,42,3,12,25,29,31,103,26,7,55,23,35,5,66,23,75,4,23,5,32,28,13,4,10,10,26,37,15,20,16,30,4,11,49,6,46,50,48,28,9,52,52,9,20,66,76,32,20,52,56,5,10,26,5,14,30,7,10,31,3,46,27,58,83,103,55,5,28,4,7,40,5,81,18,27,32,3,39,8,123,11,22,8,73,20,41,25,14,23,9,52,94,62,44,7,44,70,56,5,28,79,32,28,10,28,22,66,58,81,51,125,72,42,5,19,66,21,32,16,58,74,24,40,40,33,30,67,46,41,13,9,38,31,63,43,40,25,37,23,30,51,31,40,30,62,69,38,47,53,4,20,26,16,60,22,27,51,35,68,19,77,21,17,8,27,18,7,51,32,25,39,45,7,39,24,17,11,37,29,39,32,57,25,79,33,28,3,63,42,18,24,21,48,14,71,26,25,80,15,39,5,54,50,50,34,52,61,43,42,73,25,44,60,42,104,48,16,69,47,30,63,21,36,40,45,17,15,66,17,35,114,63,76,59,87,5,78,28,93,41,47,75,38,5,98,119,57,72,8,46,94,73,106,102,25,49,85,79,45,62,39,9,32,12,48,57,60,28,65,117,14,36,61,40,52,21,34,31,43,24,51,24,18,46,36,50,34,40,61,19,34,82,37,78,62,27,49,92,11,8,100,26,21,9,52,132,13,6,30,34,18,57,20,68,72,6,30,46,40,5,82,61,7,33,38,48,5,23,5,31,5,5,46,23,4,27,51,12,4,29,50,18,31,6,15,84,13,28,5,97,8,45,37,50,30,5,55,12,5,39,4,26,10,43,104,62,4,126,47,74,39,19,45,55,13,10,39,98,16,43,45,95,49,56,56,30,90,44,58,87,15,48,70,64,90,48,51,33,96,49,90,77,72,68,37,17,7,58,54,99,56,27,20,70,91,46,36,34,23,91,6,48,49,37,43,35,28,69,50,61,53,41,27,32,42,125,33,20,54,44,61,34,74,21,61,38,70,71,40,30,84,66,26,29,48,57,57,24,69,23,35,60,116,102,243,56,28,66,41,42,33,27,61,37,131,85,53,16,58,96,41,49,94,5,51,64,94,22,7,42,41,11,62,89,12,22,5,11,43,77,44,5,37,12,18,29,58,9,124,40,15,5,28,12,16,56,76,13,5,51,74,45,26,4,49,26,18,25,9,38,25,16,5,28,40,35,39,93,18,17,5,16,46,46,7,33,62,38,51,4,46,60,50,42,26,73,39,74,30,31,10,30,28,42,85,42,9,91,77,42,57,29,22,18,7,27,31,18,17,35,51,26,5,9,14,96,41,160,39,13,28,38,11,56,65,63,38,81,58,48,90,73,38,175,5,61,17,4,75,36,37,8,81,11,36,43,52,6,108,14,75,50,82,52,22,17,16,9,37,37,34,64,51,25,18,18,45,102,52,3,17,17,43,6,85,5,31,65,70,6,19,8,74,32,7,116,19,21,5,57,29,49,3,18,5,30,10,12,16,5,21,27,9,39,28,5,20,5,14,6,32,8,103,42,18,5,46,52,26,29,33,16,7,57,32,39,8,84,16,31,5,50,27,10,24,71,33,16,10,7,32,26,46,33,5,33,52,27,7,65,7,40,74,26,32,61,9,42,55,20,8,56,4,33,33,35,51,7,58,92,62,87,5,74,4,38,15,19,13,26,31,41,41,39,20,9,25,10,35,19,22,113,24,16,5,8,31,98,11,5,44,101,3,80,29,69,30,51,48,5,71,26,22,41,40,55,103,25,7,62,76,9,57,32,20,9,61,27,51,100,11,23,44,35,44,12,16,69,48,37,53,33,21,30,29,16,44,31,8,76,6,67,71,22,6,68,39,45,42,17,91,45,47,53,34,123,66,106,6,164,82,63,49,66,134,34,160,108,92,18,48,58,40,47,11,6,39,30,82,44,34,53,13,50,30,31,37,51,46,14,15,14,38,59,35,81,41,39,47,29,26,25,43,7,46,47,75,33,24,33,39,39,35,89,44,33,11,11,55,77,41,37,34,40,54,23,53,16,85,26,26,28,69,9,15,55,28,87,6,49,25,13,13,35,8,89,53,48,16,25,11,49,6,104,43,9,20,57,26,17,37,52,18,28,39,46,28,9,14,47,20,28,96,51,58,49,31,57,8,41,37,89,73,35,73,19,5,125,16,74,83,50,66,38,9,98,52,57,6,70,11,27,83,7,18,29,39,13,31,20,31,5,17,44,24,4,41,74,58,39,63,7,53,280,7,76,56,85,41,9,64,21,36,45,44,16,6,59,62,22,10,43,31,54,24,31,13,50,58,17,30,24,3,48,69,25,11,22,37,33,51,23,39,20,46,67,34,5,23,40,51,57,45,100,16,5,16,43,31,32,29,55,89,28,7,56,40,54,13,32,5,16,16,11,26,30,21,59,8,42,32,25,32,45,62,6,48,51,5,12,182,45,32,97,30,8,11,35,52,33,11,26,31,30,19,80,34,44,24,20,27,61,67,34,29,14,44,11,16,33,8,12,23,33,38,37,21,20,10,15,40,18,30,31,46,42,39,61,107,15,11,37,45,89,54,17,16,27,26,40,4,19,46,8,73,56,48,43,5,41,30,43,37,40,41,7,46,54,67,19,25,5,35,68,34,20,129,34,9,97,5,30,35,21,6,59,73,30,7,16,46,47,7,27,29,116,81,22,68,47,43,14,19,6,62,71,29,10,59,14,62,51,36,20,6,64,127,22,33,45,36,7,22,97,35,49,5,36,19,90,20,6,60,59,45,25,7,38,68,11,32,12,8,31,61,7,44,37,38,4,25,91,25,41,94,35,22,35,17,44,52,61,6,11,59,5,60,32,13,18,33,55,12,45,28,22,40,4,47,69,4,36,35,5,67,4,57,6,94,37,19,49,14,21,40,73,49,3,19,14,35,47,28,26,36,18,26,27,41,38,36,30,38,22,46,84,19,16,12,29,3,28,4,36,5,18,28,2,28,6,25,17,58,5,31,6,23,5,23,5,23,13,33,6,31,30,9,22,3,18,4,22,8,20,11,5,46,53,3,27,5,22,5,19,26,5,26,3,43,4,23,24,3,52,6,22,10,21,6,29,5,27,3,36,6,27,3,24,4,40,26,5,30,6,28,8,30,10,31,31,6,16,17,30,7,50,33,4,63,8,50,54,63,8,68,8,95,4,27,14,21,4,53,10,17,81,5,5,4,101,5,72,24,17,34,5,45,64,23,57,16,18,27,22,35,80,71,5,4,63,4,44,24,7,71,9,41,8,10,14,59,6,33,52,7,38,12,6,17,27,35,5,18,29,37,21,4,63,36,52,30,6,60,29,90,25,39,7,14,24,46,10,51,27,25,6,38,49,64,5,21,42,64,8,34,74,54,60,75,23,39,51,4,74,28,71,13,33,36,33,3,59,40,14,31,53,60,83,44,51,52,15,17,26,62,41,90,89,24,35,35,41,14,7,10,36,5,22,6,30,5,40,4,15,5,12,25,5,18,6,16,16,4,23,7,26,4,24,3,19,5,30,3,18,6,14,7,14,6,35,22,5,20,7,32,26,26,14,3,15,5,43,4,25,5,18,30,7,17,9,40,52,3,21,46,35,22,50,16,49,19,17,19,23,4,52,41,5,21,23,28,18,46,16,95,8,28,23,34,26,37,44,5,94,3,36,31,46,46,42,22,15,26,16,7,84,80,6,54,54,6,78,23,5,73,6,41,49,50,38,5,57,34,4,32,26,54,14,49,29,10,102,68,61,68,34,42,46,44,83,35,25,7,29,5,47,63,15,32,16,43,36,25,5,29,74,20,4,46,46,54,4,41,44,5,29,3,38,95,3,73,31,5,53,71,28,4,58,66,21,44,9,27,13,6,68,44,40,63,100,64,74,100,15,40,5,55,56,83,5,45,62,3,61,50,26,4,77,70,30,5,33,46,17,3,17,39,36,34,10,66,15,66,51,34,49,6,30,7,26,34,42,37,45,29,67,37,13,7,106,8,65,55,53,70,29,25,13,13,6,32,36,30,33,5,49,53,40,31,7,15,51,32,72,23,53,10,34,24,19,36,6,19,80,47,34,29,30,23,5,106,58,7,105,56,5,36,29,37,67,41,8,16,36,26,28,31,38,13,23,21,43,33,16,35,4,77,9,42,53,115,38,68,6,28,13,8,57,3,34,5,34,5,22,7,52,5,38,18,77,42,6,61,86,81,62,42,20,14,73,37,78,68,75,73,139,95,56,35,33,34,50,53,56,50,60,49,46,41,77,42,28,33,65,41,8,29,59,66,106,70,37,43,11,26,37,14,62,83,9,87,94,107,105,46,26,32,74,102,43,8,60,103,4,30,132,15,31,64,73,8,9,22,21,14,40,50,11,43,53,86,56,14,8,42,93,59,42,8,132,49,145,85,8,18,5,31,5,29,25,58,5,41,5,17,26,6,50,3,35,29,5,25,9,5,37,5,22,11,37,5,14,26,5,25,3,17,39,5,22,10,32,5,5,12,7,16,3,22,5,24,6,26,24,20,7,61,5,35,7,17,44,11,19,14,8,38,4,21,4,18,3,16,19,24,5,19,22,6,27,12,31,4,32,28,22,5,34,7,20,4,28,5,38,5,56,7,17,5,41,27,4,16,6,25,8,37,4,5,30,7,15,16,21,22,3,25,6,36,5,19,6,24,6,19,7,20,9,29,17,4,46,12,6,36,3,16,5,25,6,38,6,22,6,6,13,3,20,7,6,41,7,41,4,17,19,17,17,5,27,6,37,5,30,4,18,4,17,43,25,9,17,23,35,3,22,3,14,6,21,5,44,6,25,24,24,4,28,16,9,6,48,4,20,32,3,31,9,30,24,5,17,11,53,6,57,13,4,12,16,26,55,14,7,27,36,23,7,52,38,35,51,57,27,14,34,28,67,12,5,25,73,52,20,91,4,77,5,59,131,7,36,16,26,32,5,28,35,20,28,35,73,38,35,6,83,56,5,47,28,5,46,6,6,25,15,19,19,23,46,69,6,22,14,28,13,63,70,53,38,12,38,5,19,15,42,6,88,5,27,26,4,54,39,66,5,68,95,30,7,85,50,25,36,6,80,62,52,5,42,50,6,38,50,10,22,89,5,50,8,49,28,7,33,23,3,50,5,93,36,38,11,44,29,12,49,39,8,39,26,36,32,59,4,43,59,27,4,33,30,22,26,19,28,62,11,143,5,66,70,47,55,27,76,10,33,34,30,27,25,14,35,19,28,40,21,57,24,82,26,19,34,17,20,23,54,48,74,8,88,78,26,76,72,171,23,30,64,89,61,39,26,74,68,30,67,77,34,30,45,44,41,40,40,32,43,46,86,39,23,49,35,40,34,37,47,4,39,56,77,22,61,128,56,34,40,14,1,1,55,86,121,129,135,10,6,42,40,46,90,80,63,56,38,14,24,29,32,9,38,134,6,2,31,14,47,40,67,75,118,15,26,8,13,99,31,10,36,77,16,24,58,30,169,106,52,30,28,8,34,18,38,99,37,64,27,17,18,11,29,21,16,45,8,50,3,32,27,66,5,84,8,73,57,19,194,4,15,5,66,24,17,26,4,15,33,8,69,21,7,52,44,7,29,64,7,98,6,41,19,49,44,3,62,14,18,67,58,6,23,37,6,55,30,22,6,41,21,5,40,35,7,18,52,10,43,7,76,62,44,21,98,44,13,59,44,68,49,65,44,89,47,21,32,37,74,18,3,38,35,38,32,13,39,50,10,37,65,33,27,59,32,7,103,33,57,6,8,65,66,14,45,44,91,102,129,57,74,35,17,11,106,82,32,24,28,3,38,7,28,7,11,22,20,19,55,43,47,5,56,8,31,9,27,8,58,7,28,8,63,44,30,55,5,23,27,30,8,31,8,32,5,41,180,55,36,6,32,5,23,58,8,70,93,58,7,44,9,40,6,5,13,51,10,49,18,5,37,4,36,4,20,38,45,4,20,14,6,41,33,7,30,13,4,26,54,5,40,46,30,5,16,5,45,6,68,3,43,40,9,57,23,6,42,32,3,25,34,7,36,4,46,5,33,19,15,5,34,11,32,25,65,3,30,22,29,5,15,22,36,12,66,10,16,26,12,43,64,91,60,40,6,73,39,63,78,13,8,68,48,6,44,48,60,55,12,35,24,16,25,42,4,21,28,24,23,42,71,54,52,18,69,12,51,134,40,37,73,59,8,106,72,25,30,20,17,2,9,38,32,12,5,13,26,24,40,66,19,49,17,18,16,15,54,32,57,9,89,118,78,123,51,6,13,17,11,67,8,70,27,34,4,5,57,5,37,5,54,51,5,35,4,23,32,16,5,32,7,47,10,45,69,9,35,37,17,22,7,46,45,38,35,33,60,29,45,52,53,34,16,34,64,37,47,41,69,60,37,35,4,31,62,18,6,56,8,46,62,3,26,14,6,57,30,10,69,46,12,24,16,58,88,81,28,5,24,17,5,50,31,4,16,66,10,93,7,48,42,12,107,108,28,34,12,44,72,26,36,12,7,58,5,26,78,8,45,11,101,10,22,6,33,13,41,21,72,72,25,11,59,53,41,72,5,28,5,19,10,33,33,5,52,5,14,5,16,4,56,67,91,55,26,61,340,6,33,25,7,107,26,9,83,9,129,5,88,5,54,5,5,63,46,8,82,10,16,34,32,17,12,7,23,47,5,36,7,34,67,16,4,16,10,49,19,5,47,77,18,33,36,5,50,55,14,5,6,43,27,18,12,34,6,59,25,5,48,4,36,7,67,8,23,20,54,42,33,3,40,14,42,22,199,10,88,63,86,77,50,35,78,21,16,40,44,161,14,68,10,90,40,35,26,31,17,78,102,148,52,22,117,62,6,45,65,128,6,82,65,79,10,25,71,81,28,45,51,44,5,19,45,109,51,7,73,95,16,12,93,33,29,55,5,71,40,85,26,11,13,119,31,74,43,27,158,4,126,14,39,8,30,68,17,117,20,42,8,80,43,67,5,83,10,5,50,66,38,142,7,63,49,22,12,20,18,7,52,50,13,6,28,7,39,8,62,10,37,7,59,5,30,4,47,9,64,5,62,7,5,46,44,44,14,5,62,13,7,67,5,34,36,25,19,8,50,77,23,7,79,7,25,7,49,72,19,41,26,4,53,68,54,24,21,7,53,39,6,37,11,73,26,5,32,6,67,6,33,34,21,9,66,9,40,53,5,11,54,73,48,50,55,45,5,84,57,53,12,52,36,29,34,14,98,6,34,100,15,19,69,22,76,75,63,57,29,35,34,29,38,18,78,170,22,7,54,47,119,29,33,36,59,105,53,5,37,76,34,11,40,80,53,94,26,10,35,17,123,7,50,14,57,46,46,38,55,79,70,35,135,72,27,24,5,49,40,33,38,60,65,46,67,61,50,5,54,111,37,4,45,18,30,42,12,7,52,30,9,28,7,60,90,10,40,9,112,6,50,13,9,12,3,35,7,63,10,11,79,5,49,7,37,6,33,9,37,44,7,38,7,25,6,33,10,35,4,25,3,19,3,24,86,26,6,40,23,4,26,5,5,31,5,59,12,8,59,9,55,33,7,17,12,48,3,20,33,24,4,15,6,36,5,22,9,54,85,19,18,45,52,13,37,77,7,63,6,49,25,19,7,48,31,3,42,48,20,8,67,31,13,6,28,13,45,8,53,3,32,4,21,65,45,4,33,37,21,39,7,24,58,11,9,67,7,54,5,96,20,66,5,72,43,6,27,16,5,41,6,22,30,38,9,60,21,7,75,39,20,78,35,17,73,45,15,48,9,41,5,59,10,53,43,24,11,39,139,11,4,96,5,41,81,4,37,7,33,5,18,45,33,44,83,8,29,12,7,19,6,18,4,42,4,13,17,4,56,24,4,73,9,23,73,70,45,23,72,19,7,14,58,59,72,43,72,85,40,31,82,70,94,130,74,9,102,6,44,11,47,1,32,26,9,17,60,48,28,21,28,11,20,26,40,32,66,15,6,5,79,27,86,54,13,61,41,3,32,37,11,29,67,59,45,8,55,41,5,43,5,29,7,97,6,17,11,9,32,30,44,58,71,6,15,21,5,70,7,23,17,9,83,20,57,46,25,56,5,48,23,18,5,24,17,8,17,3,8,117,26,26,10,5,52,4,44,5,126,3,16,29,7,5,36,25,9,121,7,28,5,36,6,36,6,60,6,29,7,21,13,49,12,38,26,21,39,35,5,20,4,22,82,24,35,15,3,18,42,28,23,5,47,38,3,28,16,16,37,7,17,3,17,6,11,10,38,40,26,42,15,5,35,24,46,3,18,8,3,36,21,49,11,6,28,33,29,24,5,29,43,11,29,4,18,6,12,26,3,60,41,7,33,10,55,30,7,35,8,21,10,54,59,6,11,10,28,21,42,30,17,9,24,18,38,11,9,44,32,38,50,31,24,33,9,3,20,19,31,23,26,47,31,19,15,11,29,36,42,14,25,26,14,36,20,25,30,6,5,27,14,22,8,9,17,33,17,28,11,14,8,4,9,14,35,7,41,46,43,8,19,13,28,29,38,6,25,36,66,146,8,80,40,8,113,59,107,73,50,46,29,66,53,6,64,54,46,37,17,24,25,64,24,29,39,10,42,37,5,35,46,70,14,6,38,95,25,46,25,47,27,68,6,44,35,21,23,5,107,106,7,19,20,74,34,23,46,20,47,6,10,141,90,25,5,18,6,41,13,72,36,44,62,14,54,26,37,7,16,30,74,37,26,34,48,27,21,31,29,64,29,37,53,102,45,91,12,43,22,2,8,9,5,12,11,3,17,36,39,27,6,142,39,6,51,2,29,5,58,40,43,28,33,60,41,24,44,93,62,23,65,46,75,35,78,14,46,25,78,49,22,15,15,17,25,2,28,17,49,55,33,6,36,26,32,16,35,35,22,31,11,60,40,68,39,57,24,78,24,8,49,45,44,17,5,90,37,35,17,28,53,28,27,10,59,61,20,77,29,7,34,59,43,58,54,50,24,9,106,48,48,12,3,60,37,5,25,39,25,18,48,95,98,30,30,32,56,8,20,20,38,3,23,15,32,27,46,56,41,109,50,73,41,46,42,31,40,24,31,13,48,60,51,41,86,5,16,32,22,5,36,24,9,27,13,14,49,28,40,5,18,5,17,86,23,32,10,82,76,48,41,6,51,86,42,26,79,25,7,36,5,8,4,5,12,25,45,41,36,5,11,38,27,43,6,39,34,57,80,40,11,64,27,6,54,8,48,5,46,49,5,27,32,5,28,35,48,7,32,35,19,33,32,29,34,6,28,22,52,5,30,15,7,7,5,15,12,18,24,42,33,55,36,33,14,41,39,39,21,66,23,5,14,4,30,74,40,19,44,92,33,4,66,132,32,73,65,16,4,40,43,5,33,7,6,55,5,37,12,20,13,6,34,28,28,8,16,7,11,32,21,34,32,7,4,28,47,50,32,53,91,25,5,66,58,19,24,4,14,20,28,16,10,14,4,38,75,27,12,4,81,55,85,44,93,67,8,61,81,39,50,5,29,23,29,60,28,8,25,31,8,34,49,59,12,22,57,37,4,50,21,49,22,35,49,24,8,57,29,5,52,46,73,4,13,20,25,31,10,36,43,32,36,11,27,62,3,6,2,2,6,8,35,15,6,61,30,10,3,55,70,31,8,6,18,62,45,116,60,7,109,20,7,40,19,20,9,29,9,4,78,41,4,51,50,4,20,49,6,39,40,4,44,32,38,33,27,6,15,3,42,24,5,23,60,93,29,50,34,4,21,36,42,36,29,26,39,20,4,28,5,30,32,6,32,3,38,19,36,24,22,4,43,9,45,34,7,37,33,20,29,3,34,42,13,31,65,23,28,46,5,55,10,35,153,10,34,23,14,13,4,61,10,5,27,3,46,98,38,45,49,39,15,36,27,30,33,27,16,10,19,15,3,32,4,30,22,30,56,65,13,10,28,53,81,7,22,62,93,30,5,21,13,6,18,114,14,12,37,33,43,44,24,47,33,49,21,24,5,45,48,24,14,22,22,21,72,47,40,6,64,6,42,51,50,20,24,4,21,11,17,6,34,4,21,4,27,11,5,23,32,4,52,40,8,26,44,43,7,56,4,45,34,22,17,4,22,15,5,38,22,32,36,58,4,23,20,10,30,16,49,11,6,5,15,6,40,27,6,13,72,18,22,12,4,20,26,11,4,29,67,27,19,40,3,77,19,26,39,58,67,3,30,5,92,43,4,26,26,20,27,7,75,61,41,29,20,25,7,22,48,9,84,41,5,22,67,4,71,4,7,10,29,4,33,46,5,56,36,8,45,64,20,8,52,5,5,5,5,11,5,19,22,110,43,32,28,38,10,72,45,62,48,45,72,162,6,26,22,14,7,6,7,6,14,35,24,82,142,11,56,36,34,50,10,17,74,66,56,74,43,23,48,77,99,12,34,20,10,12,45,5,88,55,5,60,4,34,5,35,18,12,55,62,77,46,41,35,94,13,9,28,33,37,32,51,33,14,54,46,72,6,36,26,31,58,17,24,13,27,32,58,66,7,33,24,58,111,5,43,26,13,27,17,64,27,43,15,38,3,28,122,5,35,53,15,8,21,22,33,37,94,15,8,64,62,23,40,22,86,41,40,8,48,90,36,120,48,71,60,159,47,32,82,5,35,22,7,25,6,24,17,4,69,23,5,44,63,16,5,27,21,26,7,86,7,36,25,5,21,5,44,33,4,93,19,4,24,8,37,15,10,30,3,6,27,13,8,10,6,9,21,13,6,20,38,8,27,10,6,19,68,19,93,32,51,10,31,85,8,5,37,2,6,63,26,32,45,25,81,23,5,42,51,22,16,26,18,42,8,29,13,42,40,6,2,9,6,8,12,23,12,18,32,6,8,11,13,43,30,29,26,31,108,23,116,14,45,6,37,20,183,55,119,75,166,101,51,85,121,53,42,26,41,61,33,52,83,31,8,17,16,28,53,65,66,24,39,19,5,47,6,39,28,12,50,16,27,27,5,30,31,23,33,135,20,20,53,7,11,28,101,16,5,24,13,4,43,10,30,27,4,60,19,32,21,5,86,53,4,71,29,6,46,23,35,15,7,20,30,2,3,22,79,19,6,23,7,18,17,22,14,52,46,26,7,16,6,49,24,48,41,20,22,7,8,41,83,46,6,30,5,35,43,28,35,6,19,6,9,35,4,41,6,41,5,4,5,15,6,30,8,9,36,12,3,34,7,21,23,13,3,57,18,3,39,53,6,40,7,29,11,7,45,50,26,76,24,56,16,17,3,34,11,22,16,3,46,74,59,8,24,36,40,39,54,24,25,83,34,29,28,6,11,82,59,5,20,32,30,46,6,22,45,67,62,51,60,79,63,8,82,74,5,26,20,4,46,4,18,7,8,51,7,83,42,77,37,10,32,22,20,160,7,51,53,11,50,22,42,55,4,27,4,73,13,25,193,124,45,42,32,31,6,36,43,40,54,42,20,81,75,48,7,18,184,5,93,60,41,7,28,14,114,25,65,23,67,75,84,32,39,9,54,27,15,42,39,10,49,20,80,36,28,76,10,18,49,34,93,6,65,9,19,60,16,19,94,35,49,9,52,72,9,44,65,15,4,25,6,23,6,37,29,9,17,48,74,16,29,87,42,53,44,7,24,49,19,33,40,19,5,50,53,17,68,40,6,41,44,7,38,18,40,138,5,48,39,8,33,31,60,23,8,21,21,56,25,20,6,26,9,56,3,69,34,39,6,20,53,4,36,18,9,19,19,4,33,22,36,28,35,61,16,21,6,48,9,41,43,4,27,29,26,5,32,7,16,28,8,82,5,54,11,7,20,47,75,5,33,21,12,19,34,25,36,21,5,23,13,20,8,80,12,45,109,28,25,6,26,55,31,3,41,3,25,17,38,17,6,25,5,14,31,24,22,19,5,26,26,11,57,31,22,3,7,35,36,28,4,41,6,52,48,27,14,7,22,9,16,5,12,3,13,22,5,6,40,19,5,28,134,6,48,9,9,5,20,5,36,12,5,21,37,22,7,25,31,6,27,22,5,34,11,20,7,10,5,12,17,17,5,40,59,30,24,71,48,50,9,35,9,15,5,8,11,19,7,15,32,23,16,20,43,18,24,21,14,38,60,12,21,10,14,21,17,38,6,42,8,19,7,4,1],\"type\":\"histogram\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"title\":{\"text\":\"Distribution of word counts within data\"},\"xaxis\":{\"title\":{\"text\":\"Word Count\"}},\"yaxis\":{\"title\":{\"text\":\"Frequency\"}},\"bargap\":0.2,\"bargroupgap\":0.2},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('47881cb9-fc59-405d-b301-1bebb9fde1d7');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentences with word length of greater than 3.0 and less than 25.0 includes 31.83% of the whole!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "which clearly is not a result we were looking forward to have, next we will clean our text, numbers and oddly nonsentences considered as one, will be eliminated."
      ],
      "metadata": {
        "id": "nHmBabYeXjqP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdpUgdWghkA8"
      },
      "source": [
        "##Section B: Text preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dr652Fha6cw"
      },
      "source": [
        "###Subsection 1: tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sents = [word_tokenize(sent) for sent in data[\"sentences\"]]\n",
        "data[\"tokenized_sents\"] = tokenized_sents\n",
        "data.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "jq1BgfrFY1Kp",
        "outputId": "04113cfe-7eb6-4eff-a3d0-b1aed45ba9e2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               sentences  labels author  \\\n",
              "11781  According ly, \\na Object \\nb Verba/tnisses \\n4...       1    kan   \n",
              "18293  -And what is it destines us for this almost \\n...       2    nei   \n",
              "11702                                                Bk.       1    kan   \n",
              "16576                           156 \\nInspiration again.       2    nei   \n",
              "16770  These, moreover, desire that their requirement...       2    nei   \n",
              "\n",
              "       sentence_len_by_words  \\\n",
              "11781                     12   \n",
              "18293                     12   \n",
              "11702                      2   \n",
              "16576                      4   \n",
              "16770                     27   \n",
              "\n",
              "                                         tokenized_sents  \n",
              "11781  [According, ly, ,, a, Object, b, Verba/tnisses...  \n",
              "18293  [-And, what, is, it, destines, us, for, this, ...  \n",
              "11702                                            [Bk, .]  \n",
              "16576                       [156, Inspiration, again, .]  \n",
              "16770  [These, ,, moreover, ,, desire, that, their, r...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-696195f3-4d63-471a-9656-f2153159891b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentences</th>\n",
              "      <th>labels</th>\n",
              "      <th>author</th>\n",
              "      <th>sentence_len_by_words</th>\n",
              "      <th>tokenized_sents</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11781</th>\n",
              "      <td>According ly, \\na Object \\nb Verba/tnisses \\n4...</td>\n",
              "      <td>1</td>\n",
              "      <td>kan</td>\n",
              "      <td>12</td>\n",
              "      <td>[According, ly, ,, a, Object, b, Verba/tnisses...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18293</th>\n",
              "      <td>-And what is it destines us for this almost \\n...</td>\n",
              "      <td>2</td>\n",
              "      <td>nei</td>\n",
              "      <td>12</td>\n",
              "      <td>[-And, what, is, it, destines, us, for, this, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11702</th>\n",
              "      <td>Bk.</td>\n",
              "      <td>1</td>\n",
              "      <td>kan</td>\n",
              "      <td>2</td>\n",
              "      <td>[Bk, .]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16576</th>\n",
              "      <td>156 \\nInspiration again.</td>\n",
              "      <td>2</td>\n",
              "      <td>nei</td>\n",
              "      <td>4</td>\n",
              "      <td>[156, Inspiration, again, .]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16770</th>\n",
              "      <td>These, moreover, desire that their requirement...</td>\n",
              "      <td>2</td>\n",
              "      <td>nei</td>\n",
              "      <td>27</td>\n",
              "      <td>[These, ,, moreover, ,, desire, that, their, r...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-696195f3-4d63-471a-9656-f2153159891b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-696195f3-4d63-471a-9656-f2153159891b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-696195f3-4d63-471a-9656-f2153159891b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "distributions.word_and_freq(data,\"tokenized_sents\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "dHGeIN-FTzSJ",
        "outputId": "26aafdad-07bc-411e-ef80-a7620a37bb97"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-3d913708909e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_and_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"tokenized_sents\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-41-dd8c4a5ee173>\u001b[0m in \u001b[0;36mword_and_freq\u001b[0;34m(data, col)\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mword_and_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"tokenized_sents\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mtokenized_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tokenized_sents\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mmp_freqdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtokenized_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mtop20words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmp_freqdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'%-16s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%-16s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m'Frequency'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'%-16s'\u001b[0m \u001b[0;34m%\u001b[0m  \u001b[0;34m'% of the total'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/probability.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mCounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# Cached number of samples in this FreqDist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/collections/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    591\u001b[0m         '''\n\u001b[1;32m    592\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/probability.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \"\"\"\n\u001b[1;32m    139\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_N\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/collections/__init__.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    677\u001b[0m                     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m                 \u001b[0m_count_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN_EHVS6iSin"
      },
      "source": [
        "###Subsection 2: normalising"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "3yfLY4PDhqSy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "855e26cc-038c-4932-85a1-623792bcd104"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "stop_words = stopwords.words('english')\n",
        "stop_words[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Lu_ArAww76m"
      },
      "outputs": [],
      "source": [
        "def normalize_sentence(tokenized_sents, minimum_length=2, stopword_removal=True, lower_case=False, punctuation_removal=True):\n",
        "  \n",
        "    normalized_sents = tokenized_sents\n",
        "    \n",
        "    if stopword_removal:\n",
        "        # Remove stopwords in English and also the given domain stopwords\n",
        "        normalized_sents=[[word for word in sentence if (word.lower() not in stopwords)] for sentence in tokenized_sents ]\n",
        "\n",
        "    if punctuation_removal:\n",
        "        # Remove punctuations\n",
        "        normalized_sents=[[word for word in sentence if word not in string.punctuation] for sentence in normalized_sents ]\n",
        "\n",
        "    if lower_case:\n",
        "        # Convert everything to lowercase and filter based on a min length\n",
        "        normalized_sents=[[word.lower() for word in sentence if len(word)>minimum_length] for sentence in normalized_sents ]\n",
        "\n",
        "    elif minimum_length>1:\n",
        "        normalized_sents= [[word for word in sentence if len(word)>minimum_length] for sentence in normalized_sents ]        \n",
        "        \n",
        "    return normalized_sents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OSaHRa1lD2T"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrT90BVDa_8S"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh8pYXYibArw"
      },
      "source": [
        "###Subsection 3: Frequency analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gE6A5YZ9bIOp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojLbKKPEbIv_"
      },
      "source": [
        "###Subsection 4: stop-word elimination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bL_J82_KbOFh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaJ47YM6bOow"
      },
      "source": [
        "###Subsection 5: lemmatization and stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "103Z1ohlbTfM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFPb1QP4bWGP"
      },
      "source": [
        "##Section C: POS-tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cx_bhMj9bcLX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVzG3ZbF2fci"
      },
      "source": [
        "##Section D: author classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjfRss-DR3fu"
      },
      "source": [
        "from transformers import BertConfig, BertTokenizer\n",
        "from transformers import BertModel\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SperdZDDWKxT"
      },
      "source": [
        "### Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFpUoggdpU3x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14d527d0-2e63-4aa1-806f-193dbb30d9bd"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'device: {device}')\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda:0\n",
            "CUDA is available!  Training on GPU ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH38OJU0X7rd"
      },
      "source": [
        "# general config\n",
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 16\n",
        "VALID_BATCH_SIZE = 16\n",
        "TEST_BATCH_SIZE = 16\n",
        "\n",
        "EPOCHS = 3\n",
        "EEVERY_EPOCH = 1000\n",
        "LEARNING_RATE = 2e-5\n",
        "CLIP = 0.0\n",
        "\n",
        "MODEL_NAME_OR_PATH = 'HooshvareLab/bert-fa-base-uncased'\n",
        "OUTPUT_PATH = '/content/bert-fa-base-uncased-sentiment-taaghceh/pytorch_model.bin'\n",
        "\n",
        "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK02AC0pYIPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7696dd1e-93b6-4263-dd86-9c53e6a438ea"
      },
      "source": [
        "# create a key finder based on label 2 id and id to label\n",
        "\n",
        "label2id = {label: i for i, label in enumerate(poets)}\n",
        "id2label = {v: k for k, v in label2id.items()}\n",
        "\n",
        "print(f'label2id: {label2id}')\n",
        "print(f'id2label: {id2label}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label2id: {'khajoo': 0, 'asad': 1, 'bahar': 2, 'feyz': 3, 'asadi': 4, 'hafez': 5, 'jami': 6, 'kamal': 7, 'moulavi': 8, 'parvin': 9}\n",
            "id2label: {0: 'khajoo', 1: 'asad', 2: 'bahar', 3: 'feyz', 4: 'asadi', 5: 'hafez', 6: 'jami', 7: 'kamal', 8: 'moulavi', 9: 'parvin'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGJRNBXFYOcx"
      },
      "source": [
        "# setup the tokenizer and configuration\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
        "config = BertConfig.from_pretrained(\n",
        "    MODEL_NAME_OR_PATH, **{\n",
        "        'label2id': label2id,\n",
        "        'id2label': id2label,\n",
        "    })\n",
        "\n",
        "print(config.to_json_string())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr9L9N91gSpm"
      },
      "source": [
        "### Input Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BIajCqGgYEG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "574e4a16-0ec2-4ffa-d634-f50ad96a1d5c"
      },
      "source": [
        "idx = np.random.randint(0, len(train))\n",
        "sample_poem = train.iloc[idx]['poem']\n",
        "sample_poet = train.iloc[idx]['poet']\n",
        "\n",
        "print(f'Sample: \\n{sample_poem}\\n{sample_poet}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample: \n",
            "دیده مرغ صراحی بقدح باز کنیم\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygnLJu8uhjPT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c435c65-fe96-4cec-b3da-dd0117ce0a2f"
      },
      "source": [
        "tokens = tokenizer.tokenize(sample_poem)\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(f'  poem: {sample_poem}')\n",
        "print(f'  Coded_tokens: {tokens}')\n",
        "print(f'   Tokens: {tokenizer.convert_tokens_to_string(tokens)}')\n",
        "print(f'Token IDs: {token_ids}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  poem: دیده مرغ صراحی بقدح باز کنیم\n",
            "  Coded_tokens: ['دیده', 'مرغ', 'صراحی', 'بق', '##د', '##ح', 'باز', 'کنیم']\n",
            "   Tokens: دیده مرغ صراحی بقدح باز کنیم\n",
            "Token IDs: [4275, 6004, 96985, 8997, 2013, 2051, 3012, 3592]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgsgZ2b5h2I4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09c7d972-3025-447f-ec0c-f10d3cbb898d"
      },
      "source": [
        "encoding = tokenizer.encode_plus(\n",
        "    sample_poem,\n",
        "    max_length=32,\n",
        "    truncation=True,\n",
        "    add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "    return_token_type_ids=True,\n",
        "    return_attention_mask=True,\n",
        "    padding='max_length',\n",
        "    return_tensors='pt',  # Return PyTorch tensors\n",
        ")\n",
        "\n",
        "print(f'Keys: {encoding.keys()}\\n')\n",
        "for k in encoding.keys():\n",
        "    print(f'{k}:\\n{encoding[k]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys: dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
            "\n",
            "input_ids:\n",
            "tensor([[    2,  4275,  6004, 96985,  8997,  2013,  2051,  3012,  3592,     4,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0]])\n",
            "token_type_ids:\n",
            "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
            "attention_mask:\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pr8cRm9xiyKh"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaJBSSuMizgr"
      },
      "source": [
        "class PoemsDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, tokenizer, poems, targets=None, label_list=None, max_len=128):\n",
        "        self.poems = poems\n",
        "        self.targets = targets\n",
        "        self.has_target = isinstance(targets, list) or isinstance(targets, np.ndarray)\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "        \n",
        "        self.label_map = {label: i for i, label in enumerate(label_list)} if isinstance(label_list, list) else {}\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.poems)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        poem = str(self.poems[item])\n",
        "\n",
        "        if self.has_target:\n",
        "            target = self.targets[item]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            poem,\n",
        "            add_special_tokens=True,\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=True,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt')\n",
        "        \n",
        "        inputs = {\n",
        "            'poem': poem,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
        "        }\n",
        "\n",
        "        if self.has_target:\n",
        "            inputs['targets'] = torch.tensor(target, dtype=torch.long)\n",
        "        \n",
        "        return inputs\n",
        "\n",
        "\n",
        "def create_data_loader(x, y, tokenizer, max_len, batch_size, label_list):\n",
        "    dataset = PoemsDataset(\n",
        "        poems=x,\n",
        "        targets=y,\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=max_len, \n",
        "        label_list=label_list)\n",
        "    \n",
        "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEcefj6fkZFl"
      },
      "source": [
        "label_list = poets\n",
        "train_data_loader = create_data_loader(train['poem'].to_numpy(), train['poet'].to_numpy(), tokenizer, MAX_LEN, TRAIN_BATCH_SIZE, label_list)\n",
        "valid_data_loader = create_data_loader(valid['poem'].to_numpy(), valid['poet'].to_numpy(), tokenizer, MAX_LEN, VALID_BATCH_SIZE, label_list)\n",
        "test_data_loader = create_data_loader(test['poem'].to_numpy(), None, tokenizer, MAX_LEN, TEST_BATCH_SIZE, label_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qSxzPU2krDk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a117735c-3245-47d8-9fdc-3208a1163f3d"
      },
      "source": [
        "sample_data = next(iter(train_data_loader))\n",
        "\n",
        "print(sample_data.keys())\n",
        "\n",
        "print(sample_data['poem'])\n",
        "print(sample_data['input_ids'].shape)\n",
        "print(sample_data['input_ids'][0, :])\n",
        "print(sample_data['attention_mask'].shape)\n",
        "print(sample_data['attention_mask'][0, :])\n",
        "print(sample_data['token_type_ids'].shape)\n",
        "print(sample_data['token_type_ids'][0, :])\n",
        "print(sample_data['targets'].shape)\n",
        "print(sample_data['targets'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['poem', 'input_ids', 'attention_mask', 'token_type_ids', 'targets'])\n",
            "['نهادی دو سه پیل زی شاه پی', 'نه محض جوهر روحی که روح جوهر جانی', 'ما مردانیم شسته بر تنگ دره', 'رانی ام چونان که راند بنده ای  را پادشاهی', 'ساحران در عهد فرعون لعین', 'وزو به عالم جان تحفه گونه گون آید', 'ساقی به دست باش که غم در کمین ماست', 'بسی گوهر ز بام آویختندم', 'الا یا راهب الدیر فهل مرت بک النوق', 'دریاست دهر کشتی خویش استوار دار', 'ما یا آنیم و این دگر فرع', 'بجانب تو روان بود جانم از شوقت', 'وان جامه و رخت تو سخت نغز است', 'چنین گفت کای بخت پیشت رهی', 'چو رامین شد به بند مهر بسته', 'تا کشی گوهری از مخزن غیب']\n",
            "torch.Size([16, 128])\n",
            "tensor([    2, 13460,  2897,  3140, 14655,  3029,  3603,  3063,     4,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0])\n",
            "torch.Size([16, 128])\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "torch.Size([16, 128])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "torch.Size([16])\n",
            "tensor([4, 0, 8, 2, 8, 7, 5, 9, 0, 9, 8, 3, 2, 4, 1, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDUNgRODuOTK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "844d265f-5438-41c1-dd35-7bc8d2704df9"
      },
      "source": [
        "sample_test = next(iter(test_data_loader))\n",
        "print(sample_test.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['poem', 'input_ids', 'attention_mask', 'token_type_ids'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mv75ARn_R_Dt"
      },
      "source": [
        "class ClassifierModel(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(ClassifierModel, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained(MODEL_NAME_OR_PATH)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, 10)\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        pooled_output = self.bert(\n",
        "            input_ids=input_ids, \n",
        "            attention_mask=attention_mask, \n",
        "            token_type_ids=token_type_ids)['pooler_output']\n",
        "        \n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrObbZAdNTNl"
      },
      "source": [
        "import torch, gc\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "pt_model = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vzQGZGUmw3-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72e8cfda-9053-4d9e-d416-35fe7cd00068"
      },
      "source": [
        "pt_model = ClassifierModel(config=config)\n",
        "pt_model = pt_model.to(device)\n",
        "\n",
        "print('pt_model', type(pt_model))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at HooshvareLab/bert-fa-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pt_model <class '__main__.ClassifierModel'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name,param in pt_model.named_parameters():\n",
        "  if'classifier' not in name:\n",
        "    param.requires_grad = False\n",
        "  else:\n",
        "    print(name)"
      ],
      "metadata": {
        "id": "Lr_-sMgyTNfZ",
        "outputId": "774dd631-8370-4a43-c24c-b48871213a90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classifier.weight\n",
            "classifier.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFZQDfLlp0Sf"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e044fZSfBoKe"
      },
      "source": [
        "def simple_accuracy(y_true, y_pred):\n",
        "    return (y_true == y_pred).mean()\n",
        "\n",
        "def acc_and_f1(y_true, y_pred, average='weighted'):\n",
        "    acc = simple_accuracy(y_true, y_pred)\n",
        "    f1 = f1_score(y_true=y_true, y_pred=y_pred, average=average)\n",
        "    return {\n",
        "        \"acc\": acc,\n",
        "        \"f1\": f1,\n",
        "    }\n",
        "\n",
        "def y_loss(y_true, y_pred, losses):\n",
        "    y_true = torch.stack(y_true).cpu().detach().numpy()\n",
        "    y_pred = torch.stack(y_pred).cpu().detach().numpy()\n",
        "    y = [y_true, y_pred]\n",
        "    loss = np.mean(losses)\n",
        "\n",
        "    return y, loss\n",
        "\n",
        "\n",
        "def eval_op(model, data_loader, loss_fn):\n",
        "    model.eval()\n",
        "\n",
        "    losses = []\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for dl in tqdm(data_loader, total=len(data_loader), desc=\"Evaluation... \"):\n",
        "            \n",
        "            input_ids = dl['input_ids']\n",
        "            attention_mask = dl['attention_mask']\n",
        "            token_type_ids = dl['token_type_ids']\n",
        "            targets = dl['targets']\n",
        "\n",
        "            # move tensors to GPU if CUDA is available\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            token_type_ids = token_type_ids.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # compute predicted outputs by passing inputs to the model\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                token_type_ids=token_type_ids)\n",
        "            \n",
        "            # convert output probabilities to predicted class\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "            # calculate the batch loss\n",
        "            loss = loss_fn(outputs, targets)\n",
        "\n",
        "            # accumulate all the losses\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            y_pred.extend(preds)\n",
        "            y_true.extend(targets)\n",
        "    \n",
        "    eval_y, eval_loss = y_loss(y_true, y_pred, losses)\n",
        "    return eval_y, eval_loss\n",
        "\n",
        "\n",
        "def train_op(model, \n",
        "             data_loader, \n",
        "             loss_fn, \n",
        "             optimizer, \n",
        "             scheduler, \n",
        "             step=0, \n",
        "             print_every_step=100, \n",
        "             eval=False,\n",
        "             eval_cb=None,\n",
        "             eval_loss_min=np.Inf,\n",
        "             eval_data_loader=None, \n",
        "             clip=0.0):\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    losses = []\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    for dl in tqdm(data_loader, total=len(data_loader), desc=\"Training... \"):\n",
        "        step += 1\n",
        "\n",
        "        input_ids = dl['input_ids']\n",
        "        attention_mask = dl['attention_mask']\n",
        "        token_type_ids = dl['token_type_ids']\n",
        "        targets = dl['targets']\n",
        "\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        token_type_ids = token_type_ids.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # compute predicted outputs by passing inputs to the model\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids)\n",
        "        \n",
        "        # convert output probabilities to predicted class\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "        # calculate the batch loss\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        # accumulate all the losses\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        if clip > 0.0:\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)\n",
        "\n",
        "        # perform optimization step\n",
        "        optimizer.step()\n",
        "\n",
        "        # perform scheduler step\n",
        "        scheduler.step()\n",
        "\n",
        "        y_pred.extend(preds)\n",
        "        y_true.extend(targets)\n",
        "\n",
        "        if eval:\n",
        "            train_y, train_loss = y_loss(y_true, y_pred, losses)\n",
        "            train_score = acc_and_f1(train_y[0], train_y[1], average='weighted')\n",
        "\n",
        "            if step % print_every_step == 0:\n",
        "                eval_y, eval_loss = eval_op(model, eval_data_loader, loss_fn)\n",
        "                eval_score = acc_and_f1(eval_y[0], eval_y[1], average='weighted')\n",
        "\n",
        "                if hasattr(eval_cb, '__call__'):\n",
        "                    eval_loss_min = eval_cb(model, step, train_score, train_loss, eval_score, eval_loss, eval_loss_min)\n",
        "\n",
        "    train_y, train_loss = y_loss(y_true, y_pred, losses)\n",
        "\n",
        "    return train_y, train_loss, step, eval_loss_min\n",
        "\n",
        "def eval_callback(epoch, epochs, output_path):\n",
        "    def eval_cb(model, step, train_score, train_loss, eval_score, eval_loss, eval_loss_min):\n",
        "        statement = ''\n",
        "        statement += 'Epoch: {}/{}...'.format(epoch, epochs)\n",
        "        statement += 'Step: {}...'.format(step)\n",
        "        \n",
        "        statement += 'Train Loss: {:.6f}...'.format(train_loss)\n",
        "        statement += 'Train Acc: {:.3f}...'.format(train_score['acc'])\n",
        "\n",
        "        statement += 'Valid Loss: {:.6f}...'.format(eval_loss)\n",
        "        statement += 'Valid Acc: {:.3f}...'.format(eval_score['acc'])\n",
        "\n",
        "        print(statement)\n",
        "\n",
        "        if eval_loss <= eval_loss_min:\n",
        "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "                eval_loss_min,\n",
        "                eval_loss))\n",
        "            \n",
        "            torch.save(model.state_dict(), output_path)\n",
        "            eval_loss_min = eval_loss\n",
        "        \n",
        "        return eval_loss_min\n",
        "\n",
        "\n",
        "    return eval_cb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTWrdialDAtN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0b0fca36f0f2487eb91474c81d914e00",
            "45b1239eb7134b7f80c2108c403f47ae",
            "f027e8fabfe74716b80fdcd0d5c90db6",
            "6bf6f298cce34eb3a72c94ba6f58d23e",
            "f8ffa633a75e4769992840ed2a302518",
            "a2c75ac349344b469d5e0e9fee15ed6b",
            "509813d5b88c417f85e629c8190631b9",
            "e68e6534a9e745eca16b2d4f092c100e",
            "6ebb434a54114ceba5093dbc625c112c",
            "941644ffe7414b6280593d12ca24fc1f",
            "d4dd3b1ccfeb49458312ffb466daedca",
            "c9f040df6798460680a16b49ce6b6555",
            "f61b1f80ae90499980571c35542bad9d",
            "55503e53603c415ba31a461069700b8d",
            "78022da1346646939e560e9d28c96188",
            "dcc082ace32144a29ced2c55ed0bc8c4",
            "2ad876acb6ad42398dcc1b99737777c8",
            "0702ec60c1e144adbbb7032592636d7b",
            "0e79a1e059fb460a951f1bd536055552",
            "c16f20d4c13843c99f3cee82bf154517",
            "fdad540984bd436e86737a9a453d99fa",
            "fe1cd5002588468eaf7231380698d2d9",
            "b3bb60a715614eb2a32b4dca3f05432e",
            "f1fa0495486e441984096a4322ad556c",
            "5d0922aaab604b2783ed11b536434ea3",
            "90372806a7e94ac1868b214cc6f18e21",
            "34e4be60d32d4961ad75b4b9fbb066f9",
            "a4d5ff0be9a843d0a3d2d068a4e70407",
            "78df1e8b8ca342c285bc790311f485f4",
            "f9f1f0bd77284b8eb7122a7688f2a63c",
            "dfa20907f3e84a85967783349867185f",
            "e7d8a55ec2f44a9699cc8b0ccc0288f8",
            "ffc2a173162a4acd9aae6ca126ffc5d9",
            "06639a03f2ad49c697f8a7c1945feab6",
            "b3b8523efaf54d91bb46c35a463da680",
            "7ab847b62c284ebead52891890526f29",
            "253d606c6fb84763845a0ea7b3ca2a79",
            "481dabbd71884aa99b270b736096aa8e",
            "00f3fdd401c140459230ab688dc671d7",
            "6c36995cb447434eacc6324865a6277d",
            "6b14438627a64313bc293d104deaaf76",
            "b11692189a4a4df583fc54688c7fa1b0",
            "d2113c8428f54d0f8975912495340483",
            "522b05eec67c4be698bdb482f85508b4",
            "49ae5e3c4afc4d31868e163e158b9a55",
            "8a64a83973684292971c300d27712e58",
            "903f7b2fb0e14c678974c7dc3e567e1b",
            "8f6ff7acbcac454781102ef13debcecc",
            "e2466e0068bc4de89d3191afa1bb3d0c",
            "5e893a29dd5d42a486806ecab427bda9",
            "3bbcd407f3c3475faf0a3e46fb93cea1",
            "6d007e8ae647429487df6bbfa21f5f2c",
            "d21a2452e24a4cd8b72f4fc37f41c5fd",
            "2823181e79804f1a8cbd7db6094d9d03",
            "ce8c580baacc42b099ab50588e23740b",
            "9565940adbe740dbbb275c262cd197f0",
            "b25aa6b224ab4d159132989eac73c027",
            "73449a89ca6f4025be2b91d8bdd0596c",
            "d1ccd3afa4ce4e20bc8ec43cbee003a6",
            "e18c674d97c446a68b2a7dd4dae7d0b3",
            "12b251a9afd94693a43d9dbd3217cc3e",
            "191d2c10bdab4f3aaa02d560b0aaadf5",
            "9a27afe0218e4e628152c4b31d48c983",
            "fe9abeca49c241c4a00874319cea877a",
            "2a9fb3d6c20947a98af6f2cf3435a59b",
            "cb99f68b64674c2ea4bb9a7292f0b86e",
            "168e3736bc304901828a4ea9a777ad7c",
            "bced3d37726c42cc9dc890d9fb12b805",
            "c4d4375217c743ecb88c44890303a1d1",
            "f3e24b0c29b94625b4c4b5ffdfecfe63",
            "6691e31df3a742a69614b88e7aa807b4",
            "bbdf4c2230924ff69a50c2da126b5288",
            "0f9ae19427fa40b18a926029f46e7039",
            "5c22b38e47d34475a7c0c878360209d2",
            "43ca2475ab9d41ea837c1e9111087843",
            "a10551ed1f844e13b058900ad30de1e4",
            "40bc1802a45548438563165b9f5b4c34",
            "d24f86a9271246bebb85327bc755d514",
            "77278da01dd54f97b68c3fb4bfda3981",
            "3fec4307f18b4e1cb296bd58e87c0eeb",
            "19302633ed9c48149e3dba96b9c244d7",
            "1a0f7b85fdd0437d9ca0720154cc694d",
            "7e901b813fa445ee8acf7c323cfad1f1",
            "e42cf731e04a4388934e3da6ec6adb03",
            "767d6ac705eb4cea9fd42b6dcde83087",
            "33355f4877c74ffba73061a18836cf57",
            "0cb2f1ce8dc84882b5c21fbc0e01fde7",
            "3eba72c56e3e40f49a9b768ac42fcb85"
          ]
        },
        "outputId": "7eecc508-45de-4cde-da50-cfaa714c2be6"
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "optimizer = optim.Adam(pt_model.parameters(), lr=LEARNING_RATE)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "step = 0\n",
        "eval_loss_min = np.Inf\n",
        "history = collections.defaultdict(list)\n",
        "\n",
        "\n",
        "for epoch in tqdm(range(1, EPOCHS + 1), desc=\"Epochs... \"):\n",
        "    train_y, train_loss, step, eval_loss_min = train_op(\n",
        "        model=pt_model, \n",
        "        data_loader=train_data_loader, \n",
        "        loss_fn=loss_fn, \n",
        "        optimizer=optimizer, \n",
        "        scheduler=scheduler, \n",
        "        step=step, \n",
        "        print_every_step=EEVERY_EPOCH, \n",
        "        eval=True,\n",
        "        eval_cb=eval_callback(epoch, EPOCHS, OUTPUT_PATH),\n",
        "        eval_loss_min=eval_loss_min,\n",
        "        eval_data_loader=valid_data_loader, \n",
        "        clip=CLIP)\n",
        "    \n",
        "    train_score = acc_and_f1(train_y[0], train_y[1], average='weighted')\n",
        "    \n",
        "    eval_y, eval_loss = eval_op(\n",
        "        model=pt_model, \n",
        "        data_loader=valid_data_loader, \n",
        "        loss_fn=loss_fn)\n",
        "    \n",
        "    eval_score = acc_and_f1(eval_y[0], eval_y[1], average='weighted')\n",
        "    \n",
        "    history['train_acc'].append(train_score['acc'])\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_acc'].append(eval_score['acc'])\n",
        "    history['val_loss'].append(eval_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b0fca36f0f2487eb91474c81d914e00",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epochs... :   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9f040df6798460680a16b49ce6b6555",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training... :   0%|          | 0/3772 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b3bb60a715614eb2a32b4dca3f05432e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluation... :   0%|          | 0/943 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/3...Step: 1000...Train Loss: 2.304766...Train Acc: 0.116...Valid Loss: 2.284853...Valid Acc: 0.136...\n",
            "Validation loss decreased (inf --> 2.284853).  Saving model ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation... :   0%|          | 0/943 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "06639a03f2ad49c697f8a7c1945feab6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation... :   0%|          | 0/943 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "06639a03f2ad49c697f8a7c1945feab6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/3...Step: 2000...Train Loss: 2.289955...Train Acc: 0.135...Valid Loss: 2.268391...Valid Acc: 0.166...\n",
            "Validation loss decreased (2.284853 --> 2.268391).  Saving model ...\n",
            "Epoch: 1/3...Step: 2000...Train Loss: 2.289955...Train Acc: 0.135...Valid Loss: 2.268391...Valid Acc: 0.166...\n",
            "Validation loss decreased (2.284853 --> 2.268391).  Saving model ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation... :   0%|          | 0/943 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "49ae5e3c4afc4d31868e163e158b9a55"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation... :   0%|          | 0/943 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "49ae5e3c4afc4d31868e163e158b9a55"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/3...Step: 3000...Train Loss: 2.280431...Train Acc: 0.146...Valid Loss: 2.254539...Valid Acc: 0.180...\n",
            "Validation loss decreased (2.268391 --> 2.254539).  Saving model ...\n",
            "Epoch: 1/3...Step: 3000...Train Loss: 2.280431...Train Acc: 0.146...Valid Loss: 2.254539...Valid Acc: 0.180...\n",
            "Validation loss decreased (2.268391 --> 2.254539).  Saving model ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation... :   0%|          | 0/943 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9565940adbe740dbbb275c262cd197f0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation... :   0%|          | 0/943 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9565940adbe740dbbb275c262cd197f0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training... :   0%|          | 0/3772 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "168e3736bc304901828a4ea9a777ad7c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training... :   0%|          | 0/3772 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "168e3736bc304901828a4ea9a777ad7c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation... :   0%|          | 0/943 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d24f86a9271246bebb85327bc755d514"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation... :   0%|          | 0/943 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d24f86a9271246bebb85327bc755d514"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2/3...Step: 4000...Train Loss: 2.256819...Train Acc: 0.159...Valid Loss: 2.244747...Valid Acc: 0.181...\n",
            "Validation loss decreased (2.254539 --> 2.244747).  Saving model ...\n",
            "Epoch: 2/3...Step: 4000...Train Loss: 2.256819...Train Acc: 0.159...Valid Loss: 2.244747...Valid Acc: 0.181...\n",
            "Validation loss decreased (2.254539 --> 2.244747).  Saving model ...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-177-0b4ca49d6733>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Epochs... \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     train_y, train_loss, step, eval_loss_min = train_op(\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpt_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mdata_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-173-258aea082cc8>\u001b[0m in \u001b[0;36mtrain_op\u001b[0;34m(model, data_loader, loss_fn, optimizer, scheduler, step, print_every_step, eval, eval_cb, eval_loss_min, eval_data_loader, clip)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# compute predicted outputs by passing inputs to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         outputs = model(\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-155-825bda7ec37f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         pooled_output = self.bert(\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1017\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m         )\n\u001b[0;32m-> 1019\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1020\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    607\u001b[0m                 )\n\u001b[1;32m    608\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    610\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    538\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-177-0b4ca49d6733>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Epochs... \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     train_y, train_loss, step, eval_loss_min = train_op(\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpt_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mdata_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-173-258aea082cc8>\u001b[0m in \u001b[0;36mtrain_op\u001b[0;34m(model, data_loader, loss_fn, optimizer, scheduler, step, print_every_step, eval, eval_cb, eval_loss_min, eval_data_loader, clip)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# compute predicted outputs by passing inputs to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         outputs = model(\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-155-825bda7ec37f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         pooled_output = self.bert(\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1017\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m         )\n\u001b[0;32m-> 1019\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1020\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    607\u001b[0m                 )\n\u001b[1;32m    608\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    610\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    538\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-ZvVuRsoYRK"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlpDbg0wDqKP"
      },
      "source": [
        "def predict(model, comments, tokenizer, max_len=128, batch_size=32):\n",
        "    data_loader = create_data_loader(comments, None, tokenizer, max_len, batch_size, None)\n",
        "    \n",
        "    predictions = []\n",
        "    prediction_probs = []\n",
        "\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for dl in tqdm(data_loader, position=0):\n",
        "            input_ids = dl['input_ids']\n",
        "            attention_mask = dl['attention_mask']\n",
        "            token_type_ids = dl['token_type_ids']\n",
        "\n",
        "            # move tensors to GPU if CUDA is available\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            token_type_ids = token_type_ids.to(device)\n",
        "            \n",
        "            # compute predicted outputs by passing inputs to the model\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                token_type_ids=token_type_ids)\n",
        "            \n",
        "            # convert output probabilities to predicted class\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            prediction_probs.extend(F.softmax(outputs, dim=1))\n",
        "\n",
        "    predictions = torch.stack(predictions).cpu().detach().numpy()\n",
        "    prediction_probs = torch.stack(prediction_probs).cpu().detach().numpy()\n",
        "\n",
        "    return predictions, prediction_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRpWTfwdoWoS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "ed41d53c9c2d4293847d6cba92678bc6",
            "b4c0de9cad2c4c51b99e7312af80f309",
            "01e9320dfc554ce4962e307bd46f719d",
            "bc5e097111d942e3ba5e8d5b3e1fecdf",
            "03c2c140d77d4e5b8682d993d5b0bf27",
            "36f5270be00d4316a4761c01b7df2c8e",
            "df6d404c345345548f615b8143ea3892",
            "66d4b5c390204095a6f3177a1d198041",
            "dab64da309014ca1a57bade6d1c55e5e",
            "293931024d0c4dd7a48720fafe0ce7c2",
            "8158df6c3dc3421592b6b248040a3fa5"
          ]
        },
        "outputId": "9fb11768-a77c-4159-c48d-46325a57d33d"
      },
      "source": [
        "test_poems = test['poem'].to_numpy()\n",
        "preds, probs = predict(pt_model, test_poems, tokenizer, max_len=128)\n",
        "\n",
        "print(preds.shape, probs.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/590 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed41d53c9c2d4293847d6cba92678bc6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18860,) (18860, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNlmNh7jazk7",
        "outputId": "12e6e1b1-1d6b-4075-954d-5b5faa01dea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 5, 5, ..., 3, 4, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds_label = [id2label[t] for t in preds]\n",
        "preds_label[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PD-if3dzbK69",
        "outputId": "7d779734-a6e7-4093-b57a-b716aa31cafd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['parvin', 'hafez', 'hafez', 'asad', 'asadi']"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##F1 Score and report"
      ],
      "metadata": {
        "id": "Vzor4JVfYv38"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRL2bgDDpUG0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57f223cd-44e4-4e80-bcf0-8cd66fb03fd3"
      },
      "source": [
        "y_test, y_pred = test['poet'].values, preds\n",
        "\n",
        "print(f'F1: {f1_score(y_test, y_pred, average=\"weighted\")}')\n",
        "print(\"--------------classification_report---------------\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.18183054145230343\n",
            "--------------classification_report---------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      khajoo       0.19      0.06      0.09      1886\n",
            "        asad       0.17      0.23      0.19      1886\n",
            "       bahar       0.18      0.21      0.19      1886\n",
            "        feyz       0.20      0.20      0.20      1886\n",
            "       asadi       0.28      0.41      0.33      1886\n",
            "       hafez       0.18      0.26      0.21      1886\n",
            "        jami       0.15      0.17      0.16      1886\n",
            "       kamal       0.16      0.12      0.14      1886\n",
            "     moulavi       0.22      0.09      0.13      1886\n",
            "      parvin       0.18      0.17      0.17      1886\n",
            "\n",
            "    accuracy                           0.19     18860\n",
            "   macro avg       0.19      0.19      0.18     18860\n",
            "weighted avg       0.19      0.19      0.18     18860\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_test,y_pred))"
      ],
      "metadata": {
        "id": "d7ZQQDSDd1Iu",
        "outputId": "aa8903c4-4907-43c1-f369-635dd62886c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[108 230 135 240 211 368 175 178 102 139]\n",
            " [ 42 427 178 172 363 210 164 122  41 167]\n",
            " [ 28 186 396 131 281 199 247 122  90 206]\n",
            " [ 61 231 157 370  91 361 230 155  83 147]\n",
            " [ 23 258 233  63 769  90 154 103  29 164]\n",
            " [ 79 231 181 217 104 495 202 149  80 148]\n",
            " [ 52 200 255 146 294 210 317 132  70 210]\n",
            " [ 74 261 182 222 209 282 201 229  86 140]\n",
            " [ 34 281 270 161 189 241 237 149 178 146]\n",
            " [ 53 242 263 138 199 292 202 131  48 318]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfnICCE7g2heQ5wd39MFcV",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}